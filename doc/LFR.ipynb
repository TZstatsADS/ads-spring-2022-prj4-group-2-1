{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d960fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.optimize as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e1ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/compas-scores-two-years.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35151005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>c_days_from_compas</th>\n",
       "      <th>...</th>\n",
       "      <th>is_violent_recid</th>\n",
       "      <th>decile_score.1</th>\n",
       "      <th>score_text</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Low</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>High</td>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sex          age_cat              race  juv_fel_count  decile_score  \\\n",
       "0  Male  Greater than 45             Other              0             1   \n",
       "1  Male          25 - 45  African-American              0             3   \n",
       "2  Male     Less than 25  African-American              0             4   \n",
       "3  Male     Less than 25  African-American              0             8   \n",
       "4  Male          25 - 45             Other              0             1   \n",
       "\n",
       "   juv_misd_count  juv_other_count  priors_count  days_b_screening_arrest  \\\n",
       "0               0                0             0                     -1.0   \n",
       "1               0                0             0                     -1.0   \n",
       "2               0                1             4                     -1.0   \n",
       "3               1                0             1                      NaN   \n",
       "4               0                0             2                      NaN   \n",
       "\n",
       "   c_days_from_compas  ... is_violent_recid  decile_score.1  score_text  \\\n",
       "0                 1.0  ...                0               1         Low   \n",
       "1                 1.0  ...                1               3         Low   \n",
       "2                 1.0  ...                0               4         Low   \n",
       "3                 1.0  ...                0               8        High   \n",
       "4                76.0  ...                0               1         Low   \n",
       "\n",
       "   v_decile_score v_score_text  priors_count.1 start   end  event  \\\n",
       "0               1          Low               0     0   327      0   \n",
       "1               1          Low               0     9   159      1   \n",
       "2               3          Low               4     0    63      0   \n",
       "3               6       Medium               1     0  1174      0   \n",
       "4               1          Low               2     0  1102      0   \n",
       "\n",
       "   two_year_recid  \n",
       "0               0  \n",
       "1               1  \n",
       "2               1  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unrelated columns\n",
    "df=df.drop(columns=['id', 'name', 'first', 'last',\n",
    "                    'compas_screening_date','dob','age','c_jail_in', \n",
    "                    'c_jail_out', 'c_case_number','c_offense_date','c_charge_desc', \n",
    "                    'c_arrest_date','r_charge_desc',\n",
    "                    'r_case_number','r_charge_desc','r_offense_date', \n",
    "                    'r_jail_in', 'r_jail_out','violent_recid','vr_case_number',\n",
    "                    'vr_offense_date', 'vr_charge_desc', 'screening_date',\n",
    "                    'v_screening_date','in_custody','out_custody','r_charge_degree',\n",
    "                    'r_days_from_arrest','vr_charge_degree','type_of_assessment',\n",
    "                    'v_type_of_assessment' ])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9deb71f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sex', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
       "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
       "       'days_b_screening_arrest', 'c_days_from_compas', 'c_charge_degree',\n",
       "       'is_recid', 'is_violent_recid', 'decile_score.1', 'score_text',\n",
       "       'v_decile_score', 'v_score_text', 'priors_count.1', 'start', 'end',\n",
       "       'event', 'two_year_recid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e6c80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7214, 22)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81f73a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5915, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter only two races\n",
    "df = df[(df.race=='African-American') | (df.race=='Caucasian')]\n",
    "df = df.dropna()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59453091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical: ['sex', 'age_cat', 'race', 'c_charge_degree', 'score_text', 'v_score_text']\n",
      "numerical: ['juv_fel_count', 'decile_score', 'juv_misd_count', 'juv_other_count', 'priors_count', 'days_b_screening_arrest', 'c_days_from_compas', 'is_recid', 'is_violent_recid', 'decile_score.1', 'v_decile_score', 'priors_count.1', 'start', 'end', 'event']\n"
     ]
    }
   ],
   "source": [
    "label_column = ['two_year_recid']\n",
    "catogory_features = []\n",
    "numeric_features = []\n",
    "\n",
    "for col in df.columns.values:\n",
    "    if col in label_column:\n",
    "        continue\n",
    "    elif df[col].dtypes in ('int64', 'float64') :\n",
    "        numeric_features += [col]\n",
    "    else:\n",
    "        catogory_features += [col]\n",
    "        \n",
    "print(\"categorical:\", catogory_features)\n",
    "print(\"numerical:\", numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4ac76ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>c_days_from_compas</th>\n",
       "      <th>...</th>\n",
       "      <th>is_violent_recid</th>\n",
       "      <th>decile_score.1</th>\n",
       "      <th>score_text</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>747</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>428.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>428</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  age_cat  race  juv_fel_count  decile_score  juv_misd_count  \\\n",
       "1    1        0     0              0             3               0   \n",
       "2    1        2     0              0             4               0   \n",
       "6    1        0     1              0             6               0   \n",
       "8    0        0     1              0             1               0   \n",
       "9    1        2     1              0             3               0   \n",
       "\n",
       "   juv_other_count  priors_count  days_b_screening_arrest  c_days_from_compas  \\\n",
       "1                0             0                     -1.0                 1.0   \n",
       "2                1             4                     -1.0                 1.0   \n",
       "6                0            14                     -1.0                 1.0   \n",
       "8                0             0                     -1.0                 1.0   \n",
       "9                0             1                    428.0               308.0   \n",
       "\n",
       "   ...  is_violent_recid  decile_score.1  score_text  v_decile_score  \\\n",
       "1  ...                 1               3           1               1   \n",
       "2  ...                 0               4           1               3   \n",
       "6  ...                 0               6           2               2   \n",
       "8  ...                 0               1           1               1   \n",
       "9  ...                 1               3           1               5   \n",
       "\n",
       "   v_score_text  priors_count.1  start  end  event  two_year_recid  \n",
       "1             1               0      9  159      1               1  \n",
       "2             1               4      0   63      0               1  \n",
       "6             1              14      5   40      1               1  \n",
       "8             1               0      2  747      0               0  \n",
       "9             2               1      0  428      1               1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we replace categorical columns with numeric values\n",
    "df_num = df.copy()\n",
    "feat2name = {}\n",
    "encoders = {}\n",
    "\n",
    "# Use Label Encoder for categorical columns (including target column)\n",
    "for feature in catogory_features:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_num[feature])\n",
    "    \n",
    "    df_num[feature] = encoder.transform(df_num[feature])\n",
    "    \n",
    "    feat2name[feature] = encoder.classes_\n",
    "    encoders[feature] = encoder\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9bf0bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['African-American', 'Caucasian'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders['race'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "603597ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df_num, test_size=0.2)\n",
    "data_train, data_val= train_test_split(data_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c911325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will define some of the constants and functions mentioned in the paper\n",
    "N = df.shape[0]  # number of samples in X\n",
    "D = df.shape[1]  # Dimension of x vector\n",
    "K = 10  # Number of prototypes represented in Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f1a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(x1, x2, alpha):\n",
    "    \"\"\"\n",
    "        Calculates the euclidean distance between x1 and x2 with feature weights alpha\n",
    "        x1: First vector in X vector space (D, 1)\n",
    "        x2: Second vector in X vector space (D, 1)\n",
    "        alpha: weight vector for each of the features (D, 1)\n",
    "    \"\"\"\n",
    "    x1 = np.matrix(x1)\n",
    "    x2 = np.matrix(x2)\n",
    "    alpha = np.matrix(alpha)\n",
    "#     print(x1, x2, alpha)\n",
    "#     print(np.multiply(np.multiply((x1 - x2), (x1 - x2)),alpha))\n",
    "    return sum(np.multiply(np.multiply((x1 - x2), (x1 - x2)), alpha))[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c52fd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test \n",
    "d(np.matrix([1,2,3]).T, np.matrix([0,0,0]).T, np.matrix([1,1,2]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b74ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save time for later, we will cache the distance map between all inputs X_i \n",
    "# and current prototypes V_k\n",
    "def d_map(X, V, alpha):\n",
    "    \"\"\"\n",
    "        Returns a 2D matrix with shape (N, K) with each cell (i, j) \n",
    "            distance from input x_i to prototype v_j with weighted features\n",
    "        X: Input matrix (N, D)\n",
    "        V: Prototype matrix (K, D)\n",
    "        alpha: weight vector for each of the features (D, 1)\n",
    "    \"\"\"\n",
    "    distance_map = np.zeros((X.shape[0], V.shape[0]))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(V.shape[0]):\n",
    "            distance_map[i, j] = d(X[i, :], V[j, :], alpha)\n",
    "            \n",
    "    return distance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e714ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[162.,   8.],\n",
       "       [ 98.,   0.],\n",
       "       [ 32.,  18.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "d_map(np.matrix([[1,2],[3,4],[6,7]]), np.matrix([[10,2],[3,40]]), np.matrix([[1.0],[1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d5eaf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_nk(X, n, V, k, alpha, dist_map, summation):\n",
    "    \"\"\"\n",
    "        Calculate the prob of X_n is classified to kth prototype using softmax\n",
    "        X: Input matrix (N, D)\n",
    "        n: the nth input to calculate the prob for\n",
    "        V: prototype matrix (K, D)\n",
    "        k: the kth prototype to classify for\n",
    "        alpha: weight vector for each of the features (D, 1)\n",
    "    \"\"\"\n",
    "    p = 0\n",
    "    exponent = np.exp(-1 * dist_map[n, k])\n",
    "    p = exponent / summation\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3534b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save time later, we will cache the probs of each x mapped to k\n",
    "def M_map(X, V, alpha):\n",
    "    \"\"\"\n",
    "        Return the prob of each x mapping to a prototype v (N, K)\n",
    "        X: Input matrix (N, D)\n",
    "        V: Prototype matrix (K, D)\n",
    "        alpha: weight vector for each of the features (D, 1)\n",
    "    \"\"\"\n",
    "    M = np.zeros((X.shape[0], V.shape[0]))\n",
    "    \n",
    "    dist_map = d_map(X, V, alpha)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(V.shape[0]):\n",
    "            summation = 0\n",
    "            for k_idx in range(V.shape[0]):\n",
    "                summation += np.exp(-1 * dist_map[i, k_idx])\n",
    "            # To avoid value error\n",
    "            if (summation == 0): \n",
    "                summation = 0.000001\n",
    "            M[i, j] = M_nk(X, i, V, j, alpha, dist_map, summation)\n",
    "    return M\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "574d24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_sub_k(M_sub_map):\n",
    "    \"\"\"\n",
    "        Calculate estimated prob of mapping to k for a subset M_map. (K,)\n",
    "        M_sub_map: prob of each x mapping to a prototype (N0, K)\n",
    "    \"\"\"\n",
    "    Ms = np.zeros(M_sub_map.shape[1])\n",
    "    \n",
    "    for k in range(M_sub_map.shape[1]):\n",
    "        for n in range(M_sub_map.shape[0]):\n",
    "            Ms[k] += M_sub_map[n, k]\n",
    "        Ms[k] /= M_sub_map.shape[0]\n",
    "    return Ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14d59851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_hats(M, V):\n",
    "    \"\"\"\n",
    "        Return a matrix of reconstructed x through M \n",
    "            using each of the prototypes. (N, D)\n",
    "        M: M_map output (N, K)\n",
    "        V: Prototype matrix (K, D)\n",
    "    \"\"\"\n",
    "    return np.matmul(M, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db6a4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hats(M, w):\n",
    "    \"\"\"\n",
    "        Return matrix of final estimates of each input through M and trained w.\n",
    "        M: M_map output (N, K)\n",
    "        w: Model weight between 0 and 1 (K, 1)\n",
    "    \"\"\"\n",
    "    y_hat = np.zeros(M.shape[0])\n",
    "    for n in range(M.shape[0]):\n",
    "        for k in range(M.shape[1]):\n",
    "            y_hat[n] += (M[n, k] * w[k])\n",
    "        # Clipping estimates to (0, 1)\n",
    "        y_hat[n] = 0.000001 if y_hat[n] <= 0 else y_hat[n]\n",
    "        y_hat[n] = 0.999999 if y_hat[n] >= 1 else y_hat[n]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df127ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_x(X, x_hats):\n",
    "    \"\"\"\n",
    "        Loss term for goodness of the prototype.\n",
    "        X: input matrix (N, D)\n",
    "        x_hats: x estimates (N, D)\n",
    "    \"\"\"\n",
    "    Lx = 0\n",
    "    for n in range(X.shape[0]):\n",
    "        for d in range(X.shape[1]):\n",
    "            Lx += (X[n, d] - x_hats[n, d]) * (X[n, d] - x_hats[n, d])\n",
    "    return Lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20c28526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_y(ys, y_hats):\n",
    "    \"\"\"\n",
    "        Loss term for accuracy of the model\n",
    "        ys: Gound-truth label of X (N, 1)\n",
    "        y_hats: y estimates (N, 1)\n",
    "    \"\"\"\n",
    "    Ly = 0\n",
    "    for n in range(ys.shape[0]): \n",
    "        Ly += (-1 * ys[n] * np.log(y_hats[n]) - (1 - ys[n]) * (np.log(1 - y_hats[n])))\n",
    "    return Ly[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c937001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_z(M_sens, M_nonsens):\n",
    "    \"\"\"\n",
    "        Loss term for fairness.\n",
    "        M_sens: M_sub_k for sensitive data (1, K)\n",
    "        M_nonsens: M_sub_k for non-sensitive data (1, K)\n",
    "    \"\"\"\n",
    "    Lz= 0.0\n",
    "    \n",
    "    for k in range(M_sens.shape[0]):\n",
    "          Lz += abs(M_sens[k] - M_nonsens[k])\n",
    "    return Lz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04fe2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFR():\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data,\n",
    "        val_data,\n",
    "        label_column,\n",
    "        sensitive_column,\n",
    "        privileged_group,\n",
    "        k,\n",
    "        A_x,\n",
    "        A_y,\n",
    "        A_z\n",
    "    ):\n",
    "        self.k = k\n",
    "        self.A_x = A_x\n",
    "        self.A_y = A_y\n",
    "        self.A_z = A_z\n",
    "        \n",
    "        self.curr_iters = 0\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.label_column = label_column\n",
    "        self.sensitive_column = sensitive_column\n",
    "        self.privileged_group = privileged_group\n",
    "        \n",
    "        train_copy = train_data.copy()\n",
    "        train_copy.drop(columns=[label_column])\n",
    "        self.X = np.matrix(train_copy.to_numpy())\n",
    "        self.y = np.matrix(train_data[label_column].to_numpy()).T\n",
    "        \n",
    "        sens = train_data[sensitive_column]\n",
    "        priv_idx = np.array(np.where(sens==privileged_group))[0].flatten()\n",
    "        nonpriv_idx = np.array(np.where(sens!=privileged_group))[0].flatten()\n",
    "        self.X_plus = self.X[priv_idx,:]\n",
    "        self.y_plus = self.y[priv_idx,:]\n",
    "        self.X_minus = self.X[nonpriv_idx,:]\n",
    "        self.y_minus = self.y[nonpriv_idx,:]\n",
    "        \n",
    "    def fit(self, init_params, maxiters=1500):\n",
    "        bnd = []\n",
    "        for i, k2 in enumerate(init_params):\n",
    "            if i < self.X.shape[1] * 2 or i >= self.X.shape[1] * 2 + self.k:\n",
    "                bnd.append((None, None))\n",
    "            else:\n",
    "                bnd.append((0, 1))\n",
    "        return optim.fmin_l_bfgs_b(self.forward, x0=init_params, epsilon=1e-5, \n",
    "                          bounds = bnd, approx_grad=True, maxfun=1500, maxiter=maxiters)\n",
    "        \n",
    "    def forward(self, params, return_params=False):\n",
    "        \"\"\"\n",
    "            \n",
    "        \"\"\"\n",
    "        self.curr_iters += 1\n",
    "        \n",
    "#         print(\"N_priv\")\n",
    "\n",
    "        N_priv, D = self.X_plus.shape\n",
    "        N_nonpriv, _ = self.X_minus.shape\n",
    "\n",
    "#         print(\"Extract\")\n",
    "        # Extract all params\n",
    "        alpha_priv = params[:D].T\n",
    "        alpha_nonpriv = params[D:2*D].T\n",
    "\n",
    "        w = params[2*D:2*D+self.k]\n",
    "        V = np.matrix(params[(2*D)+self.k:]).reshape((self.k, D))\n",
    "\n",
    "#         print(\"Ms\")\n",
    "        M_k_p = M_map(self.X_plus, V, alpha_priv)\n",
    "        M_k_n = M_map(self.X_minus, V, alpha_nonpriv)\n",
    "\n",
    "#         print(\"Lz\")\n",
    "        Lz = L_z(M_sub_k(M_k_p), M_sub_k(M_k_n))\n",
    "\n",
    "#         print(\"Xhats\")\n",
    "        # To save time, we will just sum the two groups up\n",
    "        x_hats_p = x_hats(M_k_p, V)\n",
    "        x_hats_n = x_hats(M_k_n, V)\n",
    "#         print(\"Lx\")\n",
    "        L_x_p = L_x(self.X_plus, x_hats_p)\n",
    "        L_x_n = L_x(self.X_minus, x_hats_n)\n",
    "\n",
    "        Lx = L_x_p + L_x_n\n",
    "\n",
    "#         print(\"Yhats\")\n",
    "        y_hats_p = y_hats(M_k_p, w)\n",
    "        y_hats_n = y_hats(M_k_n, w)\n",
    "#         print(\"Ly\")\n",
    "        L_y_p = L_y(self.y_plus, y_hats_p)\n",
    "        L_y_n = L_y(self.y_minus, y_hats_n)\n",
    "\n",
    "        Ly = L_y_p + L_y_n\n",
    "\n",
    "        print(\"Loss\", Lx, Ly, Lz)\n",
    "        loss = (self.A_x * Lx) + (self.A_y * Ly) + (self.A_z * Lz)\n",
    "\n",
    "        if self.curr_iters % 10 == 0:\n",
    "            print(\"step:\", self.curr_iters, \n",
    "                \"loss:\", loss, \n",
    "                \"Lx:\", Lx, \n",
    "                \"Ly:\", Ly, \n",
    "                \"Lz:\", Lz)\n",
    "#             print(\"params y_hats_p, y_hats_n, M_k_p, M_k_n, loss:\",\n",
    "#                  y_hats_p, y_hats_n, M_k_p, M_k_n, loss)\n",
    "\n",
    "        if return_params:\n",
    "            return y_hats_p, y_hats_n, M_k_p, M_k_n, loss\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1dab3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "model = LFR(\n",
    "    data_train,\n",
    "    data_val,\n",
    "    \"two_year_recid\",\n",
    "    \"race\",\n",
    "    1,\n",
    "    K,\n",
    "    0.00000001,\n",
    "    0.01,\n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9453de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init_param = np.random.uniform(size=df.shape[1] * 2 + K + df.shape[1] * K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce5c1980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57.86823313805911"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(init_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0de0ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865458.7316937 2808.870817670758 0.01032227740660313\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "step: 10 loss: 57.86823313805911 Lx: 1945865459.307467 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "step: 20 loss: 57.86823313805911 Lx: 1945865459.307467 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865458.4100614 2808.8705102447225 0.010319100359677702\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "step: 30 loss: 57.86823313805911 Lx: 1945865459.307467 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "step: 40 loss: 57.86823313805911 Lx: 1945865459.307467 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8751546799076 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.875161050719 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8757501412183 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.875745271096 0.010320875539573893\n",
      "step: 50 loss: 57.868287585359525 Lx: 1945865459.307467 Ly: 2808.875745271096 Lz: 0.010320875539573893\n",
      "Loss 1945865459.307467 2808.8756203819435 0.010320875539573893\n",
      "Loss 1945865460.5382762 2808.8693714643123 0.010320622108836719\n",
      "Loss 1945865459.3034875 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3085637 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3144536 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2468135 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.314845 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3089285 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2642593 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2754025 2808.8703005410553 0.010320875539573893\n",
      "step: 60 loss: 57.86823313773847 Lx: 1945865459.2754025 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865458.821935 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.309913 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3066616 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.313859 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.24638 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3006327 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.259192 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.300396 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2618628 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.1304212 2808.8703005410553 0.010320875539573893\n",
      "step: 70 loss: 57.86823313628866 Lx: 1945865459.1304212 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865451.8760304 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.306128 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3089528 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.7729714 2808.870314820751 0.010320622788828698\n",
      "Loss 1945865459.303464 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3085642 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3144784 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2467322 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3148446 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.308927 2808.8703005410553 0.010320875539573893\n",
      "step: 80 loss: 57.86823313807372 Lx: 1945865459.308927 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.2641897 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2753425 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865458.8212876 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3099122 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3066626 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.313854 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2463002 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3006287 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.259098 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3003988 2808.8703005410553 0.010320875539573893\n",
      "step: 90 loss: 57.86823313798843 Lx: 1945865459.3003988 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.261795 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.1301398 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865451.8660235 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3061204 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3089662 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3441243 2808.8702231750603 0.010320791076315755\n",
      "Loss 1945865459.3028684 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3087227 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3154182 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.237935 2808.8703005410553 0.010320875539573893\n",
      "step: 100 loss: 57.868233137363795 Lx: 1945865459.237935 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.3158727 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3090146 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2564993 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.269804 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865458.7539663 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3103528 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3064294 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3146467 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.237452 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2997057 2808.8703005410553 0.010320875539573893\n",
      "step: 110 loss: 57.868233137981505 Lx: 1945865459.2997057 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.2515936 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2993593 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2538154 2808.8703005410553 0.010320875539573893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945865459.0977006 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865450.8814406 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3058872 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.309085 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.1657982 2808.870336869979 0.010320821205368452\n",
      "Loss 1945865459.3029048 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3086433 2808.8703005410553 0.010320875539573893\n",
      "step: 120 loss: 57.86823313807088 Lx: 1945865459.3086433 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.3154535 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2383049 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.315909 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3090348 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2569904 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2702274 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865458.756614 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.310304 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3064718 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.314598 2808.8703005410553 0.010320875539573893\n",
      "step: 130 loss: 57.86823313813042 Lx: 1945865459.314598 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.2378318 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2997537 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2519646 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2993946 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2543025 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.0998662 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865450.907524 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.305902 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3091192 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.4014635 2808.8700445478153 0.01032088081568916\n",
      "step: 140 loss: 57.86823585518194 Lx: 1945865459.4014635 Ly: 2808.8700445478153 Lz: 0.01032088081568916\n",
      "Loss 1945865459.3030958 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3086286 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.315195 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.240415 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3156455 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3090727 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2589612 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2716956 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865458.7725625 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3101735 2808.8703005410553 0.010320875539573893\n",
      "step: 150 loss: 57.86823313808618 Lx: 1945865459.3101735 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.306525 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.314477 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2399352 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2999277 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2538257 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.299621 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.2563527 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.1082954 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865451.1302843 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945865459.3059306 2808.8703005410553 0.010320875539573893\n",
      "step: 160 loss: 57.86823313804375 Lx: 1945865459.3059306 Ly: 2808.8703005410553 Lz: 0.010320875539573893\n",
      "Loss 1945865459.3090892 2808.8703005410553 0.010320875539573893\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.4856832 2792.5656131636974 0.14007291903867522\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "step: 170 loss: 187.45812055964183 Lx: 1945859687.87098 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "step: 180 loss: 187.45812055964183 Lx: 1945859687.87098 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.2785885 2792.565765453176 0.14007495465728376\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "step: 190 loss: 187.45812055964183 Lx: 1945859687.87098 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "step: 200 loss: 187.45812055964183 Lx: 1945859687.87098 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.5704191503432 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.570417734388 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.57102544634 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.5709649829914 0.1400738693759037\n",
      "step: 210 loss: 187.4581759044434 Lx: 1945859687.87098 Ly: 2792.5709649829914 Lz: 0.1400738693759037\n",
      "Loss 1945859687.87098 2792.5707187886082 0.1400738693759037\n",
      "Loss 1945859689.249915 2792.5645818436096 0.14007106462125438\n",
      "Loss 1945859687.866931 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8713431 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8779635 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8127239 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.878434 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8724556 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8303707 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8432236 2792.565430502833 0.1400738693759037\n",
      "step: 220 loss: 187.45812055936426 Lx: 1945859687.8432236 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.4064069 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8733807 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.870318 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8773131 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.812241 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8641334 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8244972 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8639526 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8279786 2792.565430502833 0.1400738693759037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945859687.7043686 2792.565430502833 0.1400738693759037\n",
      "step: 230 loss: 187.4581205579757 Lx: 1945859687.7043686 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859680.4322872 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8697598 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8725739 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859688.5146658 2792.5653286061424 0.14007106324531876\n",
      "Loss 1945859687.8669367 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8713436 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8779635 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8127556 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.878434 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.872456 2792.565430502833 0.1400738693759037\n",
      "step: 240 loss: 187.45812055965658 Lx: 1945859687.872456 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.8304038 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8432126 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.4066935 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8733797 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.870307 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8773127 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8122735 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.864133 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8245819 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8639555 2792.565430502833 0.1400738693759037\n",
      "step: 250 loss: 187.45812055957157 Lx: 1945859687.8639555 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.8280106 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.704497 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859680.4349585 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8697712 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8725748 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.907185 2792.5654396386526 0.1400738181544452\n",
      "Loss 1945859687.866281 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8729186 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8790092 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.7976656 2792.565430502833 0.1400738693759037\n",
      "step: 260 loss: 187.45812055890866 Lx: 1945859687.7976656 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.8795943 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8724003 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8161392 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8283849 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.286485 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8738546 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8696852 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8782015 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.7971475 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.862946 2792.565430502833 0.1400738693759037\n",
      "step: 270 loss: 187.45812055956148 Lx: 1945859687.862946 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.812319 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8626304 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8136237 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.6457856 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859679.3169596 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8692813 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.872357 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.5955114 2792.5655383116346 0.14007415310193724\n",
      "Loss 1945859687.8663392 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8727844 2792.565430502833 0.1400738693759037\n",
      "step: 280 loss: 187.45812055965985 Lx: 1945859687.8727844 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.8789372 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.7992902 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8795533 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8724117 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8178058 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8298016 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.298098 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8738256 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8697567 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8780913 2792.565430502833 0.1400738693759037\n",
      "step: 290 loss: 187.45812055971294 Lx: 1945859687.8780913 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.7987483 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.863037 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8136022 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8626983 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8152807 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.6527085 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859679.422338 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8693104 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8724284 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859688.1803572 2792.5649841163045 0.1400747113322719\n",
      "step: 300 loss: 187.45895805523853 Lx: 1945859688.1803572 Ly: 2792.5649841163045 Lz: 0.1400747113322719\n",
      "Loss 1945859687.866571 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8721304 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8785126 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8054042 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8789945 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8724437 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8236334 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8356798 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.3462796 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8736184 2792.565430502833 0.1400738693759037\n",
      "step: 310 loss: 187.4581205596682 Lx: 1945859687.8736184 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.870039 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.87773 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.804895 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8635507 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8185906 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8633177 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.8211508 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.6767051 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859679.8726227 2792.565430502833 0.1400738693759037\n",
      "Loss 1945859687.869536 2792.565430502833 0.1400738693759037\n",
      "step: 320 loss: 187.4581205596274 Lx: 1945859687.869536 Ly: 2792.565430502833 Lz: 0.1400738693759037\n",
      "Loss 1945859687.8724868 2792.565430502833 0.1400738693759037\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.4649587 2803.7041663049886 0.026871469086054367\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "step: 330 loss: 74.3684739449514 Lx: 1945863217.0117497 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "step: 340 loss: 74.3684739449514 Lx: 1945863217.0117497 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.177928 2803.703956078324 0.02687443530852443\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "step: 350 loss: 74.3684739449514 Lx: 1945863217.0117497 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "step: 360 loss: 74.3684739449514 Lx: 1945863217.0117497 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.708620875045 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.7086255566287 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.7091384821633 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.709141460249 0.026872804632397662\n",
      "step: 370 loss: 74.36852821711766 Lx: 1945863217.0117497 Ly: 2803.709141460249 Lz: 0.026872804632397662\n",
      "Loss 1945863217.0117497 2803.7090297030054 0.026872804632397662\n",
      "Loss 1945863218.3189282 2803.7028393761266 0.026872430702933492\n",
      "Loss 1945863217.0077353 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0126958 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.018674 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9518037 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0192149 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0131502 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9692454 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9808707 2803.703714243625 0.026872804632397662\n",
      "step: 380 loss: 74.36847394464262 Lx: 1945863216.9808707 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863216.532404 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0142028 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0110059 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0180347 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9513652 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0048618 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.964038 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.004689 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.966887 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.8376112 2803.703714243625 0.026872804632397662\n",
      "step: 390 loss: 74.36847394321002 Lx: 1945863216.8376112 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863209.594962 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0104632 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0132918 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.5098352 2803.7037293030676 0.026872430773274752\n",
      "Loss 1945863217.0077338 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0126967 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.018674 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9517233 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0192196 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0131474 2803.703714243625 0.026872804632397662\n",
      "step: 400 loss: 74.36847394496539 Lx: 1945863217.0131474 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863216.9691887 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9808145 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.5318074 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0142019 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.011015 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0180354 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9512992 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0048604 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9639852 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0046883 2803.703714243625 0.026872804632397662\n",
      "step: 410 loss: 74.36847394488079 Lx: 1945863217.0046883 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863216.9668365 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.8373437 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863209.5867407 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0104628 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0132923 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.051146 2803.703612759695 0.026872881491746964\n",
      "Loss 1945863217.0071528 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0131292 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0196385 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9414494 2803.703714243625 0.026872804632397662\n",
      "step: 420 loss: 74.36847394424841 Lx: 1945863216.9414494 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863217.0202632 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.013198 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9599833 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9730053 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.4517155 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0146148 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.010629 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0189178 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9409723 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.003903 2803.703714243625 0.026872804632397662\n",
      "step: 430 loss: 74.36847394487295 Lx: 1945863217.003903 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863216.9552927 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0035644 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9573574 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.7988093 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863208.5630941 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0101213 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0133572 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.851579 2803.703727762194 0.026872908556408864\n",
      "Loss 1945863217.0071824 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0130327 2803.703714243625 0.026872804632397662\n",
      "step: 440 loss: 74.36847394496424 Lx: 1945863217.0130327 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863217.0196502 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9419606 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0202193 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.013247 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9606175 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9736032 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.455469 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0146024 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0106544 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.018899 2803.703714243625 0.026872804632397662\n",
      "step: 450 loss: 74.3684739450229 Lx: 1945863217.018899 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863216.941486 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0039043 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9557738 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0036268 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9580088 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.801622 2803.703714243625 0.026872804632397662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945863208.5930789 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0101 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0133605 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.116714 2803.703476458162 0.026872951904452658\n",
      "step: 460 loss: 74.36861884020142 Lx: 1945863217.116714 Ly: 2803.703476458162 Lz: 0.026872951904452658\n",
      "Loss 1945863217.0073125 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0128865 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0194552 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9446998 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0200047 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0132263 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9632087 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9757977 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.4762418 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0144854 2803.703714243625 0.026872804632397662\n",
      "step: 470 loss: 74.36847394497876 Lx: 1945863217.0144854 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863217.010799 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0187485 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9442115 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0041428 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9580925 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0039017 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.9606433 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863216.8124278 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863208.8449264 2803.703714243625 0.026872804632397662\n",
      "Loss 1945863217.0102072 2803.703714243625 0.026872804632397662\n",
      "step: 480 loss: 74.36847394493599 Lx: 1945863217.0102072 Ly: 2803.703714243625 Lz: 0.026872804632397662\n",
      "Loss 1945863217.0133424 2803.703714243625 0.026872804632397662\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.2883954 2807.4765600604433 0.0017112952078859878\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "step: 490 loss: 49.24393733252647 Lx: 1945864831.8572588 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "step: 500 loss: 49.24393733252647 Lx: 1945864831.8572588 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864830.9759097 2807.476277433191 0.0017095876467911053\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "step: 510 loss: 49.24393733252647 Lx: 1945864831.8572588 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "step: 520 loss: 49.24393733252647 Lx: 1945864831.8572588 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.4809281022062 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.4809340344236 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.4815000220106 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.4814978441896 0.0017105284211650917\n",
      "step: 530 loss: 49.24399171817958 Lx: 1945864831.8572588 Ly: 2807.4814978441896 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8572588 2807.4813777763743 0.0017105284211650917\n",
      "Loss 1945864833.1100464 2807.475145426415 0.0017104708842041183\n",
      "Loss 1945864831.8532753 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8582544 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8642125 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.7968001 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8647122 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8587365 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8142824 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8255053 2807.476059278879 0.0017105284211650917\n",
      "step: 540 loss: 49.24393733220894 Lx: 1945864831.8255053 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.3733196 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8597288 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.85645 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.863574 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.7963917 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8503852 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8091507 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8501582 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8118668 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.680974 2807.476059278879 0.0017105284211650917\n",
      "step: 550 loss: 49.24393733076362 Lx: 1945864831.680974 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864824.4303846 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8559709 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8587828 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864832.3312297 2807.476074775005 0.0017104712107408326\n",
      "Loss 1945864831.853272 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8582544 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.864213 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.79671 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8647125 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8587353 2807.476059278879 0.0017105284211650917\n",
      "step: 560 loss: 49.243937332541236 Lx: 1945864831.8587353 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.814213 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8254478 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.3726795 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.859724 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.856451 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8635783 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.7963297 2807.476059278879 0.0017105284211650917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864831.850378 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8090813 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8501563 2807.476059278879 0.0017105284211650917\n",
      "step: 570 loss: 49.243937332455445 Lx: 1945864831.8501563 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8117824 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.6806922 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864824.420762 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8559692 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8587816 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8947225 2807.4759741352727 0.0017105815806288116\n",
      "Loss 1945864831.8526878 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.858467 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8651552 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.787534 2807.476059278879 0.0017105284211650917\n",
      "step: 580 loss: 49.24393733182922 Lx: 1945864831.787534 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8657305 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.858786 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8060982 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.819322 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.302034 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8601675 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8562095 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8643909 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.7870612 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8494763 2807.476059278879 0.0017105284211650917\n",
      "step: 590 loss: 49.243937332448645 Lx: 1945864831.8494763 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8012414 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8491664 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8034067 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.646645 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864823.425483 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8556771 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8588636 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.7110343 2807.4760882772493 0.0017104546864719627\n",
      "Loss 1945864831.8527417 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8584595 2807.476059278879 0.0017105284211650917\n",
      "step: 600 loss: 49.24393733253848 Lx: 1945864831.8584595 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8651998 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.787942 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.865686 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8587856 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8066316 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8197818 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.3049428 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8601503 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8562236 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.864328 2807.476059278879 0.0017105284211650917\n",
      "step: 610 loss: 49.24393733259716 Lx: 1945864831.864328 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.7874727 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8495116 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8016498 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8491898 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.803957 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.6489656 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864823.4521236 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8556619 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8588722 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.9525187 2807.4758104965067 0.0017105105149556599\n",
      "step: 620 loss: 49.24391693944591 Lx: 1945864831.9525187 Ly: 2807.4758104965067 Lz: 0.0017105105149556599\n",
      "Loss 1945864831.852848 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8583689 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8649507 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.7901752 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8654609 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.858844 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8087566 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8214679 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.322034 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8600597 2807.476059278879 0.0017105284211650917\n",
      "step: 630 loss: 49.24393733255448 Lx: 1945864831.8600597 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.8563285 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.86418 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.7897167 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8496766 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8036075 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.8494554 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.806124 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.657964 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864823.6815262 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945864831.855725 2807.476059278879 0.0017105284211650917\n",
      "step: 640 loss: 49.24393733251114 Lx: 1945864831.855725 Ly: 2807.476059278879 Lz: 0.0017105284211650917\n",
      "Loss 1945864831.858869 2807.476059278879 0.0017105284211650917\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.3193917 2801.178726788104 0.010691037167351564\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "step: 650 loss: 58.162850962970964 Lx: 1945863138.8920093 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "step: 660 loss: 58.162850962970964 Lx: 1945863138.8920093 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.0120459 2801.1784681362624 0.010694169155959415\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "step: 670 loss: 58.162850962970964 Lx: 1945863138.8920093 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "step: 680 loss: 58.162850962970964 Lx: 1945863138.8920093 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1830552040547 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.183059965425 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.183604132483 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.1836040842563 0.010692437146314238\n",
      "step: 690 loss: 58.1629045760769 Lx: 1945863138.8920093 Ly: 2801.1836040842563 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8920093 2801.183491984457 0.010692437146314238\n",
      "Loss 1945863140.175829 2801.1773542264546 0.010692343576912455\n",
      "Loss 1945863138.888047 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8929543 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8989975 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8318577 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8994708 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8934386 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8493 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8606634 2801.1782427736634 0.010692437146314238\n",
      "step: 700 loss: 58.1628509626575 Lx: 1945863138.8606634 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.4107432 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8944335 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8912582 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8983274 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8314288 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8851657 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.844158 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8849025 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.846881 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.7169266 2801.1782427736634 0.010692437146314238\n",
      "step: 710 loss: 58.16285096122013 Lx: 1945863138.7169266 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863131.4810355 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.890709 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8935533 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863139.3786244 2801.1782589849 0.010692343374412827\n",
      "Loss 1945863138.8880467 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.892955 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8990216 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8317752 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8994708 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8934364 2801.1782427736634 0.010692437146314238\n",
      "step: 720 loss: 58.16285096298523 Lx: 1945863138.8934364 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8492413 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8606143 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.4101794 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.894432 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.891244 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8983297 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8313632 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8851695 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8440862 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8848946 2801.1782427736634 0.010692437146314238\n",
      "step: 730 loss: 58.16285096289981 Lx: 1945863138.8848946 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8468199 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.716683 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863131.4729233 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.890708 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8935568 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.9220204 2801.178159128419 0.010692518137274232\n",
      "Loss 1945863138.8873954 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8932898 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8999848 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8220062 2801.1782427736634 0.010692437146314238\n",
      "step: 740 loss: 58.16285096227094 Lx: 1945863138.8220062 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.900417 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8934722 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8406167 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.853725 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.334539 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.894853 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.890902 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8991785 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.821539 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8842134 2801.1782427736634 0.010692437146314238\n",
      "step: 750 loss: 58.162850962893 Lx: 1945863138.8842134 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8357778 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8838334 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8379154 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.6804442 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863130.447296 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8903491 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8935761 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.7471294 2801.178259112095 0.010692518824864883\n",
      "Loss 1945863138.8874192 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.893196 2801.1782427736634 0.010692437146314238\n",
      "step: 760 loss: 58.16285096298283 Lx: 1945863138.893196 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.899948 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.822445 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.9004335 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8934937 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8411093 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8541567 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.3374298 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8948183 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8909545 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.899152 2801.1782427736634 0.010692437146314238\n",
      "step: 770 loss: 58.16285096304239 Lx: 1945863138.899152 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8219652 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8842363 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8361673 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.883841 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8384433 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.682651 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863130.4730744 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8904028 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8936312 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.9901562 2801.1780060406886 0.010692515317397733\n",
      "step: 780 loss: 58.16292676770618 Lx: 1945863138.9901562 Ly: 2801.1780060406886 Lz: 0.010692515317397733\n",
      "Loss 1945863138.8875546 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8931546 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8997216 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8248348 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.9001815 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8935332 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8434215 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8560867 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.3559306 2801.1782427736634 0.010692437146314238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945863138.8947406 2801.1782427736634 0.010692437146314238\n",
      "step: 790 loss: 58.16285096299828 Lx: 1945863138.8947406 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.891041 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8988988 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8243906 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.88446 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8382826 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8840957 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8407893 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.692348 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863130.709742 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945863138.8905084 2801.1782427736634 0.010692437146314238\n",
      "step: 800 loss: 58.16285096295596 Lx: 1945863138.8905084 Ly: 2801.1782427736634 Lz: 0.010692437146314238\n",
      "Loss 1945863138.8936553 2801.1782427736634 0.010692437146314238\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.5432172 2806.6962514280203 0.0019911181621614094\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "step: 810 loss: 49.51789603802293 Lx: 1945864623.112584 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "step: 820 loss: 49.51789603802293 Lx: 1945864623.112584 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.2313733 2806.6959717796717 0.001993783309512276\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "step: 830 loss: 49.51789603802293 Lx: 1945864623.112584 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "step: 840 loss: 49.51789603802293 Lx: 1945864623.112584 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.7006146400536 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.7006204257304 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.701183558367 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.7011817021194 0.001992292279747593\n",
      "step: 850 loss: 49.51795032789463 Lx: 1945864623.112584 Ly: 2806.7011817021194 Lz: 0.001992292279747593\n",
      "Loss 1945864623.112584 2806.7010626766487 0.001992292279747593\n",
      "Loss 1945864624.3691764 2806.6948420343715 0.0019923461430370604\n",
      "Loss 1945864623.1085935 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1136208 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1195486 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0521588 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1200821 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1140482 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0696154 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0808914 2806.6957527149502 0.001992292279747593\n",
      "step: 860 loss: 49.51789603770601 Lx: 1945864623.0808914 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864622.6290565 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1150575 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1118515 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1188164 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0517292 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1056921 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.064499 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.105501 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0672143 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.9365 2806.6957527149502 0.001992292279747593\n",
      "step: 870 loss: 49.517896036262094 Lx: 1945864622.9365 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864615.6876802 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.111248 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1141033 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.5877728 2806.695768348881 0.001992345881025648\n",
      "Loss 1945864623.108578 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.113622 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.119554 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0520754 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1200843 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1140518 2806.6957527149502 0.001992292279747593\n",
      "step: 880 loss: 49.51789603803761 Lx: 1945864623.1140518 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.0695367 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.080844 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.6284094 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1150537 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1118536 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1188202 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.051651 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1056807 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0644336 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1054986 2806.6957527149502 0.001992292279747593\n",
      "step: 890 loss: 49.51789603795208 Lx: 1945864623.1054986 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.0671349 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.936225 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864615.6782432 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.111239 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1141038 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1489062 2806.695667684554 0.0019924066914740746\n",
      "Loss 1945864623.107983 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.11381 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.120492 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0428262 2806.6957527149502 0.001992292279747593\n",
      "step: 900 loss: 49.51789603732536 Lx: 1945864623.0428262 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864623.1210766 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1141028 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0614352 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0745955 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.5571256 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1154366 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1115625 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1196475 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.042348 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.104777 2806.6957527149502 0.001992292279747593\n",
      "step: 910 loss: 49.51789603794487 Lx: 1945864623.104777 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.056535 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1044939 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.058713 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.9018905 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864614.6792216 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1110106 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.114194 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.9665737 2806.695780069668 0.00199238380091743\n",
      "Loss 1945864623.108024 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1137528 2806.6957527149502 0.001992292279747593\n",
      "step: 920 loss: 49.51789603803463 Lx: 1945864623.1137528 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.120389 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0432374 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1210637 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1141472 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0619278 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0750842 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.5600164 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1154547 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1115947 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1196516 2806.6957527149502 0.001992292279747593\n",
      "step: 930 loss: 49.51789603809361 Lx: 1945864623.1196516 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.0427647 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.10481 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0569391 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1045148 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0592487 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.9041855 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864614.705758 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1109567 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1142156 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.208095 2806.6955055377625 0.0019923058485607215\n",
      "step: 940 loss: 49.517907136019296 Lx: 1945864623.208095 Ly: 2806.6955055377625 Lz: 0.0019923058485607215\n",
      "Loss 1945864623.1081429 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.113708 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.120223 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.045498 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1207793 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.114176 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0640697 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0767899 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.5773196 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.115346 2806.6957527149502 0.001992292279747593\n",
      "step: 950 loss: 49.51789603805056 Lx: 1945864623.115346 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.111681 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1194508 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0450287 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1050115 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0589182 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.104726 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.0614376 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864622.9132829 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864614.936029 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864623.1110735 2806.6957527149502 0.001992292279747593\n",
      "step: 960 loss: 49.51789603800783 Lx: 1945864623.1110735 Ly: 2806.6957527149502 Lz: 0.001992292279747593\n",
      "Loss 1945864623.1141882 2806.6957527149502 0.001992292279747593\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.124034 2807.2066790470244 0.0014467234028112397\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "step: 970 loss: 48.976665196190424 Lx: 1945864759.6930914 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "step: 980 loss: 48.976665196190424 Lx: 1945864759.6930914 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864758.8118575 2807.206397450233 0.0014450152113930081\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "step: 990 loss: 48.976665196190424 Lx: 1945864759.6930914 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "step: 1000 loss: 48.976665196190424 Lx: 1945864759.6930914 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.211045420755 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.211051302409 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.211616297715 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.211614232914 0.0014459558094534197\n",
      "step: 1010 loss: 48.976719548713476 Lx: 1945864759.6930914 Ly: 2807.211614232914 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.6930914 2807.2114945273142 0.0014459558094534197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864760.9470887 2807.205266226314 0.0014459033611242955\n",
      "Loss 1945864759.6890678 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6941984 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.700104 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6326165 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7004814 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6945095 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6501207 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6613498 2807.206178980609 0.0014459558094534197\n",
      "step: 1020 loss: 48.97666519587301 Lx: 1945864759.6613498 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.2093482 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6955462 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.692325 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6994205 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6321907 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6862073 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.644999 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6860113 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.647676 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.516859 2807.206178980609 0.0014459558094534197\n",
      "step: 1030 loss: 48.97666519442811 Lx: 1945864759.516859 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864752.2668934 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6917486 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6946402 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864760.167345 2807.2061945258797 0.0014459036786212964\n",
      "Loss 1945864759.689063 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6941974 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7001045 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6325731 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7004836 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6945076 2807.206178980609 0.0014459558094534197\n",
      "step: 1040 loss: 48.97666519620459 Lx: 1945864759.6945076 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.6500504 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6613214 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.208696 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6955466 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6923275 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6994212 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.632123 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.686212 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6449234 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6860027 2807.206178980609 0.0014459558094534197\n",
      "step: 1050 loss: 48.97666519611954 Lx: 1945864759.6860027 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.647606 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.516583 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864752.2573376 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.691746 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.694641 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7300086 2807.2060938738878 0.001446009738078069\n",
      "Loss 1945864759.6884747 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6943746 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7010465 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6233497 2807.206178980609 0.0014459558094534197\n",
      "step: 1060 loss: 48.97666519549301 Lx: 1945864759.6233497 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.7014968 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6946316 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6419396 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6551423 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.137792 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6959581 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6920304 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.700172 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6228757 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6853106 2807.206178980609 0.0014459558094534197\n",
      "step: 1070 loss: 48.97666519611262 Lx: 1945864759.6853106 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.637063 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.68497 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6392226 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.4824991 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864751.2607646 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6914597 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6946902 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.5469363 2807.2062074080322 0.0014458802123620829\n",
      "Loss 1945864759.6885238 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6943903 2807.206178980609 0.0014459558094534197\n",
      "step: 1080 loss: 48.97666519620341 Lx: 1945864759.6943903 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.7010536 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6237583 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7014925 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6946688 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6424508 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.655596 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.1407304 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6959596 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.692061 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7001712 2807.206178980609 0.0014459558094534197\n",
      "step: 1090 loss: 48.97666519626122 Lx: 1945864759.7001712 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.6232648 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6853247 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6374655 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6850195 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6397588 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.4848108 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864751.2873669 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.691481 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.694747 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.788394 2807.2059307568943 0.001445933831344448\n",
      "step: 1100 loss: 48.97664073679733 Lx: 1945864759.788394 Ly: 2807.2059307568943 Lz: 0.001445933831344448\n",
      "Loss 1945864759.6886296 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.694291 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7008 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6260054 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7012749 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6945658 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6445966 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.657291 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.1578999 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6958482 2807.206178980609 0.0014459558094534197\n",
      "step: 1110 loss: 48.97666519621799 Lx: 1945864759.6958482 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.692143 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.7000124 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6255276 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6855211 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.639442 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6852696 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6419516 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.4937954 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864751.5170686 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864759.6915727 2807.206178980609 0.0014459558094534197\n",
      "step: 1120 loss: 48.97666519617524 Lx: 1945864759.6915727 Ly: 2807.206178980609 Lz: 0.0014459558094534197\n",
      "Loss 1945864759.6947255 2807.206178980609 0.0014459558094534197\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864738.5786762 2807.129866558354 0.0013795949599081647\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "step: 1130 loss: 48.909456425607566 Lx: 1945864739.1476974 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "step: 1140 loss: 48.909456425607566 Lx: 1945864739.1476974 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.266314 2807.129585255013 0.0013794374203858484\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "step: 1150 loss: 48.909456425607566 Lx: 1945864739.1476974 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "step: 1160 loss: 48.909456425607566 Lx: 1945864739.1476974 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1342324570096 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.134238324081 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1348030380223 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.134801005167 0.0013795153671749827\n",
      "step: 1170 loss: 48.90951076870363 Lx: 1945864739.1476974 Ly: 2807.134801005167 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.1476974 2807.1346814022854 0.0013795153671749827\n",
      "Loss 1945864740.4021397 2807.128454253657 0.0013794596211036403\n",
      "Loss 1945864739.1436555 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1488128 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.154655 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0872614 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1550932 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1491835 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1047149 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1159542 2807.1293666955607 0.0013795153671749827\n",
      "step: 1180 loss: 48.90945642529013 Lx: 1945864739.1159542 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864738.6639214 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1501632 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1468937 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1540017 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0868206 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1408253 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0995975 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1406176 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.102315 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.9714942 2807.1293666955607 0.0013795153671749827\n",
      "step: 1190 loss: 48.90945642384553 Lx: 1945864738.9714942 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864731.721702 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1463213 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.149181 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.6219792 2807.1293822545977 0.0013795526498350252\n",
      "Loss 1945864739.1436338 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1488128 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1546564 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.087183 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1550949 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1491876 2807.1293666955607 0.0013795153671749827\n",
      "step: 1200 loss: 48.909456425622466 Lx: 1945864739.1491876 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.104641 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1159065 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.663282 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1501641 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1468916 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1540008 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0867445 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.140819 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0995255 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1406116 2807.1293666955607 0.0013795153671749827\n",
      "step: 1210 loss: 48.90945642553671 Lx: 1945864739.1406116 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.1022508 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.9712155 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864731.7121592 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1463237 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1491895 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.184507 2807.129281599834 0.0013796110026430275\n",
      "Loss 1945864739.1430664 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1489267 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1555922 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0779405 2807.1293666955607 0.0013795153671749827\n",
      "step: 1220 loss: 48.90945642491 Lx: 1945864739.0779405 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.1560874 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1492324 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0965192 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1097229 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.5923529 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1505547 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.146657 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1548114 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0774775 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.139892 2807.1293666955607 0.0013795153671749827\n",
      "step: 1230 loss: 48.90945642552951 Lx: 1945864739.139892 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.0916529 2807.1293666955607 0.0013795153671749827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864739.139585 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0938408 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.9370568 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864730.715219 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1460347 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1492612 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0013404 2807.1293949611268 0.0013794731330997179\n",
      "Loss 1945864739.1430938 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.148932 2807.1293666955607 0.0013795153671749827\n",
      "step: 1240 loss: 48.909456425619915 Lx: 1945864739.148932 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.1556096 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0783656 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1561065 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1492567 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.097033 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1101778 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.5952635 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1505022 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1466594 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1548727 2807.1293666955607 0.0013795153671749827\n",
      "step: 1250 loss: 48.909456425679316 Lx: 1945864739.1548727 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.0778852 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.13994 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.092071 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1396556 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0943828 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.9393435 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864730.741818 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1460829 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1492863 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.2428637 2807.1291186302606 0.0013795098034238285\n",
      "step: 1260 loss: 48.90944838215508 Lx: 1945864739.2428637 Ly: 2807.1291186302606 Lz: 0.0013795098034238285\n",
      "Loss 1945864739.143233 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1488795 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1553993 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0806274 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1558666 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1491985 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0991702 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1118622 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.6124344 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1504307 2807.1293666955607 0.0013795153671749827\n",
      "step: 1270 loss: 48.90945642563489 Lx: 1945864739.1504307 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.146749 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1546583 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.080144 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1401496 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.0940464 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1398911 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.096581 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864738.94837 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864730.9716012 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864739.1461997 2807.1293666955607 0.0013795153671749827\n",
      "step: 1280 loss: 48.90945642559259 Lx: 1945864739.1461997 Ly: 2807.1293666955607 Lz: 0.0013795153671749827\n",
      "Loss 1945864739.1492918 2807.1293666955607 0.0013795153671749827\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.2019157 2806.631830396386 0.0014030614887898807\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "step: 1290 loss: 48.929196432189336 Lx: 1945864723.771099 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "step: 1300 loss: 48.929196432189336 Lx: 1945864723.771099 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864722.8895025 2806.631549072645 0.0014057272044811775\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "step: 1310 loss: 48.929196432189336 Lx: 1945864723.771099 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "step: 1320 loss: 48.929196432189336 Lx: 1945864723.771099 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6361904939404 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.636195815864 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.636759782181 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6367577853653 0.0014042358816662992\n",
      "step: 1330 loss: 48.92925069723095 Lx: 1945864723.771099 Ly: 2806.6367577853653 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.771099 2806.6366385285073 0.0014042358816662992\n",
      "Loss 1945864725.0255558 2806.630420607031 0.0014042977679950275\n",
      "Loss 1945864723.7671099 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7720838 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7780771 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7106543 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.778492 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7725058 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7280955 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7393818 2806.6313312812044 0.0014042358816662992\n",
      "step: 1340 loss: 48.929196431872164 Lx: 1945864723.7393818 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864723.2872996 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7735624 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7703185 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7774186 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.710219 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7642617 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7230115 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7639816 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.725707 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.5948768 2806.6313312812044 0.0014042358816662992\n",
      "step: 1350 loss: 48.92919643042712 Lx: 1945864723.5948768 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864716.3449438 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7697597 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7726345 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864724.2456627 2806.6313468771737 0.0014042975192736784\n",
      "Loss 1945864723.767106 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7720852 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.778088 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7105782 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7785277 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7725039 2806.6313312812044 0.0014042358816662992\n",
      "step: 1360 loss: 48.92919643220339 Lx: 1945864723.7725039 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.7280402 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7393243 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.2867198 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7735598 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7703185 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7774217 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.710157 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7642462 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7229378 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7639816 2806.6313312812044 0.0014042358816662992\n",
      "step: 1370 loss: 48.92919643211817 Lx: 1945864723.7639816 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.7256432 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.594637 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864716.3362732 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7697587 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7726367 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.8075347 2806.631246638027 0.0014043499545606797\n",
      "Loss 1945864723.7665472 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.772225 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.779025 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.701347 2806.6313312812044 0.0014042358816662992\n",
      "step: 1380 loss: 48.929196431491825 Lx: 1945864723.701347 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.7795534 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7726114 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7199209 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7331352 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.2157345 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.773975 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7700288 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.778274 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7008486 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7633128 2806.6313312812044 0.0014042358816662992\n",
      "step: 1390 loss: 48.929196432111475 Lx: 1945864723.7633128 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.715076 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7629237 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7172337 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.560443 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864715.338488 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.769473 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7727027 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.6251929 2806.6313590910186 0.0014043253496689445\n",
      "Loss 1945864723.7665253 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.772211 2806.6313312812044 0.0014042358816662992\n",
      "step: 1400 loss: 48.92919643220046 Lx: 1945864723.772211 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.7790625 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7017522 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7794847 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7726574 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.720438 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7335823 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.218646 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7739677 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7700772 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7782583 2806.6313312812044 0.0014042358816662992\n",
      "step: 1410 loss: 48.92919643226093 Lx: 1945864723.7782583 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.7012892 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.763367 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7154648 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7629676 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7177777 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.5627503 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864715.3649836 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7694688 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7727075 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.8664474 2806.6310838254994 0.0014042553197370355\n",
      "step: 1420 loss: 48.92921339665651 Lx: 1945864723.8664474 Ly: 2806.6310838254994 Lz: 0.0014042553197370355\n",
      "Loss 1945864723.766701 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7721827 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7787998 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7040243 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7792764 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.772594 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7225845 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7352722 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.2358222 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.77386 2806.6313312812044 0.0014042358816662992\n",
      "step: 1430 loss: 48.92919643221695 Lx: 1945864723.77386 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.7701502 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7780712 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7035422 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7635674 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.717456 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7632499 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7199612 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.571795 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864715.594801 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864723.7695765 2806.6313312812044 0.0014042358816662992\n",
      "step: 1440 loss: 48.929196432174116 Lx: 1945864723.7695765 Ly: 2806.6313312812044 Lz: 0.0014042358816662992\n",
      "Loss 1945864723.7727342 2806.6313312812044 0.0014042358816662992\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864734.8897462 2807.0104051148983 0.0013770024037501882\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "step: 1450 loss: 48.90566925152485 Lx: 1945864735.4588091 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "step: 1460 loss: 48.90566925152485 Lx: 1945864735.4588091 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864734.5775056 2807.010123806612 0.0013768449447439934\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "step: 1470 loss: 48.90566925152485 Lx: 1945864735.4588091 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "step: 1480 loss: 48.90566925152485 Lx: 1945864735.4588091 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.014769622401 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.014775358707 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.0153398933858 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.01533786921 0.0013769228426233626\n",
      "step: 1490 loss: 48.90572357590356 Lx: 1945864735.4588091 Ly: 2807.01533786921 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.4588091 2807.015218349342 0.0013769228426233626\n",
      "Loss 1945864736.7133784 2807.0089934133184 0.0013768673805874265\n",
      "Loss 1945864735.4547763 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4598904 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4658103 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.398346 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4662309 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.460268 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4158213 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4270918 2807.0099054313396 0.0013769228426233626\n",
      "step: 1500 loss: 48.90566925120768 Lx: 1945864735.4270918 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864734.9750812 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4612703 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4580708 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4651632 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.3979282 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4518917 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4106965 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4517155 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4134388 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.2826009 2807.0099054313396 0.0013769228426233626\n",
      "step: 1510 loss: 48.905669249762774 Lx: 1945864735.2826009 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864728.0327692 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4574814 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4603097 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.9331832 2807.0099209992995 0.001376959941769812\n",
      "Loss 1945864735.45477 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4598851 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4658103 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.398277 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4662337 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4602687 2807.0099054313396 0.0013769228426233626\n",
      "step: 1520 loss: 48.90566925153945 Lx: 1945864735.4602687 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.4157557 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.427028 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864734.9744465 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4612465 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.458053 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4651976 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.3978531 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4518785 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4106288 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4517152 2807.0099054313396 0.0013769228426233626\n",
      "step: 1530 loss: 48.905669251453915 Lx: 1945864735.4517152 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.4133708 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.2823305 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864728.0234408 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4574764 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4603095 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4957604 2807.009820444224 0.001377018476025088\n",
      "Loss 1945864735.4542165 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4601054 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4667544 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.3890486 2807.0099054313396 0.0013769228426233626\n",
      "step: 1540 loss: 48.90566925082725 Lx: 1945864735.3890486 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.4672842 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4603453 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4076216 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4208536 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864734.9034653 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4616067 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4578013 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.466011 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.388596 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4509988 2807.0099054313396 0.0013769228426233626\n",
      "step: 1550 loss: 48.905669251446746 Lx: 1945864735.4509988 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.4027739 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4506545 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4049659 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.2481923 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864727.0262914 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.457189 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.460395 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.3125787 2807.0099335875498 0.0013768805360367675\n",
      "Loss 1945864735.4542632 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4600875 2807.0099054313396 0.0013769228426233626\n",
      "step: 1560 loss: 48.90566925153764 Lx: 1945864735.4600875 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864735.4667578 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.3894632 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4672468 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.460342 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4081702 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4212918 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864734.9064364 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4616115 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4577906 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4659877 2807.0099054313396 0.0013769228426233626\n",
      "step: 1570 loss: 48.90566925159664 Lx: 1945864735.4659877 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.3890057 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4510345 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4031844 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4506981 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.405503 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.2504783 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864727.0528693 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4572248 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4604485 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.5541077 2807.0096575123202 0.0013769171444329709\n",
      "step: 1580 loss: 48.90566107509725 Lx: 1945864735.5541077 Ly: 2807.0096575123202 Lz: 0.0013769171444329709\n",
      "Loss 1945864735.4543793 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4599938 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.466426 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.391724 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4669762 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4603634 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.410291 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.422998 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864734.9236119 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4615207 2807.0099054313396 0.0013769228426233626\n",
      "step: 1590 loss: 48.90566925155196 Lx: 1945864735.4615207 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.457899 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.465827 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.3912556 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4512427 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4051588 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4509647 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4076748 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.2594776 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864727.282662 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864735.4572875 2807.0099054313396 0.0013769228426233626\n",
      "step: 1600 loss: 48.905669251509636 Lx: 1945864735.4572875 Ly: 2807.0099054313396 Lz: 0.0013769228426233626\n",
      "Loss 1945864735.460395 2807.0099054313396 0.0013769228426233626\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.1741848 2806.760501993911 0.0013854573615036392\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "step: 1610 loss: 48.912878963250364 Lx: 1945864727.7433357 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "step: 1620 loss: 48.912878963250364 Lx: 1945864727.7433357 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864726.8617768 2806.76022067536 0.0013881228904601972\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "step: 1630 loss: 48.912878963250364 Lx: 1945864727.7433357 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "step: 1640 loss: 48.912878963250364 Lx: 1945864727.7433357 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.764863590651 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.7648690533256 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.7654332129223 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.765431206799 0.0013866316589616068\n",
      "step: 1650 loss: 48.91293324846295 Lx: 1945864727.7433357 Ly: 2806.765431206799 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.7433357 2806.7653118605385 0.0013866316589616068\n",
      "Loss 1945864728.9977694 2806.7590915544433 0.001386693804704825\n",
      "Loss 1945864727.739305 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7443526 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7503002 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6829016 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.750826 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.744743 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7003503 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7116141 2806.76000268554 0.0013866316589616068\n",
      "step: 1660 loss: 48.91287896293315 Lx: 1945864727.7116141 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.2595434 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7457857 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7426124 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7495456 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6824932 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7365253 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.695238 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7362587 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6979544 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.567121 2806.76000268554 0.0013866316589616068\n",
      "step: 1670 loss: 48.91287896148822 Lx: 1945864727.567121 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864720.317225 2806.76000268554 0.0013866316589616068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864727.742052 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7448297 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864728.2178454 2806.760018271967 0.001386693549235124\n",
      "Loss 1945864727.739357 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7443528 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7503257 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.682827 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7508276 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7448206 2806.76000268554 0.0013866316589616068\n",
      "step: 1680 loss: 48.912878963265214 Lx: 1945864727.7448206 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.700289 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7115667 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.258951 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7457862 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7426014 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7495449 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.682416 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.736524 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.695159 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7362564 2806.76000268554 0.0013866316589616068\n",
      "step: 1690 loss: 48.912878963179566 Lx: 1945864727.7362564 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.6979053 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.566863 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864720.3083286 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7420483 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7448235 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7799835 2806.759917925454 0.001386745756334351\n",
      "Loss 1945864727.7387319 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.744522 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7512388 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6735928 2806.76000268554 0.0013866316589616068\n",
      "step: 1700 loss: 48.91287896255293 Lx: 1945864727.6735928 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.7518208 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7448359 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6921468 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7053266 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.1879716 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7462096 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7423148 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.750397 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6731157 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7355967 2806.76000268554 0.0013866316589616068\n",
      "step: 1710 loss: 48.91287896317297 Lx: 1945864727.7355967 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.6872907 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7352433 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6894786 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.5327005 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864719.310751 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7417235 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7449222 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.5975187 2806.7600306129652 0.0013867210560099819\n",
      "Loss 1945864727.7387595 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7444925 2806.76000268554 0.0013866316589616068\n",
      "step: 1720 loss: 48.91287896326193 Lx: 1945864727.7444925 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.7511578 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6740074 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7518039 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7448812 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6926956 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7057872 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.190949 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7461581 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.742332 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.750382 2806.76000268554 0.0013866316589616068\n",
      "step: 1730 loss: 48.91287896332082 Lx: 1945864727.750382 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.6735537 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7356102 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6877058 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7352204 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6900158 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.5349665 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864719.3372865 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7417736 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7449813 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.8384817 2806.759755072325 0.0013866512893319638\n",
      "step: 1740 loss: 48.91289611844003 Lx: 1945864727.8384817 Ly: 2806.759755072325 Lz: 0.0013866512893319638\n",
      "Loss 1945864727.7388728 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.74445 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.750974 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6762571 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7515204 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7447991 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6948175 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7074893 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.2080603 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7460537 2806.76000268554 0.0013866316589616068\n",
      "step: 1750 loss: 48.91287896327754 Lx: 1945864727.7460537 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.742412 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7501972 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.675828 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7357929 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.6896787 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7355058 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.692217 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.5440195 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864719.5670972 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864727.7418227 2806.76000268554 0.0013866316589616068\n",
      "step: 1760 loss: 48.91287896323523 Lx: 1945864727.7418227 Ly: 2806.76000268554 Lz: 0.0013866316589616068\n",
      "Loss 1945864727.7449422 2806.76000268554 0.0013866316589616068\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.4760847 2806.9322311692817 0.0013753061539041878\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "step: 1770 loss: 48.90319125994634 Lx: 1945864733.0452852 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "step: 1780 loss: 48.90319125994634 Lx: 1945864733.0452852 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.1638956 2806.9319498577615 0.0013751487476353075\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "step: 1790 loss: 48.90319125994634 Lx: 1945864733.0452852 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "step: 1800 loss: 48.90319125994634 Lx: 1945864733.0452852 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.9365947664064 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.936600416997 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.937164834476 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.937162815843 0.0013752266134628155\n",
      "step: 1810 loss: 48.9032455720741 Lx: 1945864733.0452852 Ly: 2806.937162815843 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.0452852 2806.937043350341 0.0013752266134628155\n",
      "Loss 1945864734.2996547 2806.930819862454 0.001375171337354375\n",
      "Loss 1945864733.041305 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.046366 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0522447 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9848545 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.052691 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.046707 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0022962 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0135665 2806.9317316030674 0.0013752266134628155\n",
      "step: 1820 loss: 48.90319125962915 Lx: 1945864733.0135665 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864732.561513 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0477142 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0445256 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0516424 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9844012 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0384738 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9971905 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0381832 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9999022 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.869086 2806.9317316030674 0.0013752266134628155\n",
      "step: 1830 loss: 48.90319125818435 Lx: 1945864732.869086 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864725.619227 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0439296 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0467849 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.5197434 2806.931747176824 0.0013752635924935142\n",
      "Loss 1945864733.0413043 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0463662 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0522394 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9847765 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0526934 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0467072 2806.9317316030674 0.0013752266134628155\n",
      "step: 1840 loss: 48.903191259960565 Lx: 1945864733.0467072 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.002231 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.013517 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.5608964 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0477166 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0445254 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0516443 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9843297 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0384722 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.997132 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0381813 2806.9317316030674 0.0013752266134628155\n",
      "step: 1850 loss: 48.9031912598753 Lx: 1945864733.0381813 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864732.9998388 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.8688219 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864725.6100326 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0439293 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.046763 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.08189 2806.931646686946 0.0013753222455774872\n",
      "Loss 1945864733.0406861 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0464907 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0531874 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.975551 2806.9317316030674 0.0013752266134628155\n",
      "step: 1860 loss: 48.903191259249 Lx: 1945864732.975551 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.0537643 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.046826 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9941196 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0073454 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.489934 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0480843 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0442357 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0524547 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9750705 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.037523 2806.9317316030674 0.0013752266134628155\n",
      "step: 1870 loss: 48.90319125986872 Lx: 1945864733.037523 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864732.989268 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0371761 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9914303 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.8346233 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864724.612751 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0436559 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0468273 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.8992574 2806.931759687665 0.0013751842593932029\n",
      "Loss 1945864733.0407147 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0465164 2806.9317316030674 0.0013752266134628155\n",
      "step: 1880 loss: 48.90319125995865 Lx: 1945864733.0465164 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.0531907 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.975948 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0536795 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0468278 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9946277 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0077791 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.4928355 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0480955 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0442972 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0524373 2806.9317316030674 0.0013752266134628155\n",
      "step: 1890 loss: 48.90319126001786 Lx: 1945864733.0524373 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864732.9754739 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0375776 2806.9317316030674 0.0013752266134628155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864732.989649 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0371993 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9919677 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.8369255 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864724.6393147 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.043697 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0468397 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.1404448 2806.931483779634 0.0013752208272428401\n",
      "step: 1900 loss: 48.90318299644363 Lx: 1945864733.1404448 Ly: 2806.931483779634 Lz: 0.0013752208272428401\n",
      "Loss 1945864733.0408401 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0464704 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0529175 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.978211 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0534644 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0467439 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.996749 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.009459 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.5100186 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0480137 2806.9317316030674 0.0013752266134628155\n",
      "step: 1910 loss: 48.903191259973624 Lx: 1945864733.0480137 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.0443673 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.052317 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9777246 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0377784 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9916372 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0374174 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.9941573 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864732.8459778 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864724.8691075 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864733.0437534 2806.9317316030674 0.0013752266134628155\n",
      "step: 1920 loss: 48.90319125993102 Lx: 1945864733.0437534 Ly: 2806.9317316030674 Lz: 0.0013752266134628155\n",
      "Loss 1945864733.0468864 2806.9317316030674 0.0013752266134628155\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864728.9764783 2806.8188809002268 0.0013775726196531002\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "step: 1930 loss: 48.905478691504165 Lx: 1945864729.5456352 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "step: 1940 loss: 48.905478691504165 Lx: 1945864729.5456352 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864728.6644092 2806.8185995841277 0.0013801387711038782\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "step: 1950 loss: 48.905478691504165 Lx: 1945864729.5456352 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "step: 1960 loss: 48.905478691504165 Lx: 1945864729.5456352 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.823243177048 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8232487036576 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8238129509596 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.8238109405133 0.0013786475810058374\n",
      "step: 1970 loss: 48.905532985867325 Lx: 1945864729.5456352 Ly: 2806.8238109405133 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.5456352 2806.823691553778 0.0013786475810058374\n",
      "Loss 1945864730.7999923 2806.8174701659073 0.0013787098444534573\n",
      "Loss 1945864729.5416613 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.546682 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5525553 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4851952 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5530708 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5471253 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5026302 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5139036 2806.8183815041975 0.0013786475810058374\n",
      "step: 1980 loss: 48.90547869118685 Lx: 1945864729.5139036 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.0618792 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5480804 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5449018 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5519986 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.484767 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5387585 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4975276 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.538515 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5002694 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.3694136 2806.8183815041975 0.0013786475810058374\n",
      "step: 1990 loss: 48.90547868974195 Lx: 1945864729.3694136 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864722.1195412 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.54431 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5471168 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864730.019999 2806.8183970863347 0.001378709585901583\n",
      "Loss 1945864729.5416522 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5466824 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5525584 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4851174 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5530689 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5471225 2806.8183815041975 0.0013786475810058374\n",
      "step: 2000 loss: 48.905478691519036 Lx: 1945864729.5471225 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.50257 2806.8183815041975 0.0013786475810058374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864729.5138578 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.0612638 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5480802 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5448866 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5519993 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4846666 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5387568 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.497476 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5385103 2806.8183815041975 0.0013786475810058374\n",
      "step: 2010 loss: 48.90547869143292 Lx: 1945864729.5385103 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.5002205 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.3691528 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864722.1105433 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.544269 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5471544 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5824375 2806.81829669109 0.0013787616895900578\n",
      "Loss 1945864729.5410838 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5468173 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5534968 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4758973 2806.8183815041975 0.0013786475810058374\n",
      "step: 2020 loss: 48.90547869080679 Lx: 1945864729.4758973 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.554072 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.547172 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4944522 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5076265 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864728.9902842 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5484853 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5446403 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5528316 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4754186 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5378532 2806.8183815041975 0.0013786475810058374\n",
      "step: 2030 loss: 48.905478691426346 Lx: 1945864729.5378532 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.4896386 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.53751 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4918008 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.3350415 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864721.1130686 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5440085 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5472507 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.399734 2806.818409485114 0.0013787369458729548\n",
      "Loss 1945864729.5410972 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.546854 2806.8183815041975 0.0013786475810058374\n",
      "step: 2040 loss: 48.90547869151635 Lx: 1945864729.546854 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.5535026 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4762897 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.55407 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.547148 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4949737 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5080857 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864728.9932027 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5484505 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5446255 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.55275 2806.8183815041975 0.0013786475810058374\n",
      "step: 2050 loss: 48.90547869157531 Lx: 1945864729.55275 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.4758348 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5378819 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4900126 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5375443 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4923482 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.3372722 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864721.139607 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5440173 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5472288 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.6409254 2806.8181338194618 0.00137866729864905\n",
      "step: 2060 loss: 48.90549593325292 Lx: 1945864729.6409254 Ly: 2806.8181338194618 Lz: 0.00137866729864905\n",
      "Loss 1945864729.5412393 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5467718 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5532873 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.478575 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.553842 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.547192 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.497098 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5098133 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.0103936 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5483675 2806.8183815041975 0.0013786475810058374\n",
      "step: 2070 loss: 48.905478691531485 Lx: 1945864729.5483675 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.544736 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5525799 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.478098 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5380561 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4919708 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5377955 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.4945374 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.3463259 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864721.3694124 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864729.5440793 2806.8183815041975 0.0013786475810058374\n",
      "step: 2080 loss: 48.90547869148861 Lx: 1945864729.5440793 Ly: 2806.8183815041975 Lz: 0.0013786475810058374\n",
      "Loss 1945864729.5472202 2806.8183815041975 0.0013786475810058374\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.1644726 2806.889750412747 0.0013743844832893315\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "step: 2090 loss: 48.90184478053901 Lx: 1945864731.7336626 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "step: 2100 loss: 48.90184478053901 Lx: 1945864731.7336626 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864730.852326 2806.8894690995307 0.001374503346553424\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "step: 2110 loss: 48.90184478053901 Lx: 1945864731.7336626 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "step: 2120 loss: 48.90184478053901 Lx: 1945864731.7336626 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8941135151085 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.894119119253 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.894683472901 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8946814574488 0.0013743049540992647\n",
      "step: 2130 loss: 48.90189908601038 Lx: 1945864731.7336626 Ly: 2806.8946814574488 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.7336626 2806.8945620214217 0.0013743049540992647\n",
      "Loss 1945864732.9881234 2806.8883393205338 0.0013742497790262542\n",
      "Loss 1945864731.7296786 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7346752 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7406507 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6732082 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.741047 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7351377 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6906662 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.701938 2806.8892509103116 0.0013743049540992647\n",
      "step: 2140 loss: 48.90184478022176 Lx: 1945864731.701938 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.2499418 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7361593 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7328749 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.739988 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.672789 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.726812 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6855567 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7265816 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6882768 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.5574622 2806.8892509103116 0.0013743049540992647\n",
      "step: 2150 loss: 48.901844778777004 Lx: 1945864731.5574622 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864724.30759 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7323055 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7351916 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864732.2080941 2806.8892664872137 0.0013743418678388575\n",
      "Loss 1945864731.7296538 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7346697 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7407336 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6731257 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7410536 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.735137 2806.8892509103116 0.0013743049540992647\n",
      "step: 2160 loss: 48.90184478055375 Lx: 1945864731.735137 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.6906056 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7018955 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.2493303 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7361598 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7328749 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7399895 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6726928 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.726801 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6854954 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.726575 2806.8892509103116 0.0013743049540992647\n",
      "step: 2170 loss: 48.90184478046813 Lx: 1945864731.726575 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.688199 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.557198 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864724.298467 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7323048 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.735192 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7705126 2806.8891660329014 0.0013744005855072516\n",
      "Loss 1945864731.7290716 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7348392 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7415571 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6639118 2806.8892509103116 0.0013743049540992647\n",
      "step: 2180 loss: 48.901844779841504 Lx: 1945864731.6639118 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.7421064 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7352014 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6825032 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6956573 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.1783648 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7365103 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.732584 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.740821 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.663437 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.725862 2806.8892509103116 0.0013743049540992647\n",
      "step: 2190 loss: 48.901844780461 Lx: 1945864731.725862 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.6776175 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7255883 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6797915 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.522994 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864723.3011143 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7319949 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7352576 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.5876985 2806.8892789560923 0.0013742625742124426\n",
      "Loss 1945864731.7290978 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.734863 2806.8892509103116 0.0013743049540992647\n",
      "step: 2200 loss: 48.90184478055101 Lx: 1945864731.734863 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.7415729 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6643286 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7420435 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.735232 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6830063 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6961288 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.18129 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7365346 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7326355 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7408 2806.8892509103116 0.0013743049540992647\n",
      "step: 2210 loss: 48.90184478061038 Lx: 1945864731.7408 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.6638436 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.725913 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6780372 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7256007 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6803014 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.5253148 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864723.3276687 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7320518 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7352514 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.8289773 2806.889003138842 0.0013742991200818844\n",
      "step: 2220 loss: 48.901836469760084 Lx: 1945864731.8289773 Ly: 2806.889003138842 Lz: 0.0013742991200818844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1945864731.7292113 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7347732 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7413688 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.666566 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.741862 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7352037 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6851513 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6977992 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.198461 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7364295 2806.8892509103116 0.0013743049540992647\n",
      "step: 2230 loss: 48.90184478056668 Lx: 1945864731.7364295 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.7327204 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7406259 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6661084 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7261257 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6799846 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.725833 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.6825123 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.5343566 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864723.5574725 2806.8892509103116 0.0013743049540992647\n",
      "Loss 1945864731.7320924 2806.8892509103116 0.0013743049540992647\n",
      "step: 2240 loss: 48.901844780523305 Lx: 1945864731.7320924 Ly: 2806.8892509103116 Lz: 0.0013743049540992647\n",
      "Loss 1945864731.7352674 2806.8892509103116 0.0013743049540992647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.40707566, 0.99818669, 0.37776356, 0.02457241, 0.85368152,\n",
       "        0.80249874, 0.10567061, 0.99269347, 0.32229838, 0.52379687,\n",
       "        0.06323416, 0.34775917, 0.39255252, 0.54815334, 0.09903947,\n",
       "        0.00743698, 0.8740434 , 0.82716246, 0.56983807, 0.04096182,\n",
       "        0.04261611, 0.20196329, 0.32885059, 0.4990336 , 0.71969098,\n",
       "        0.56915806, 0.89067285, 0.39984768, 0.24177958, 0.41771883,\n",
       "        0.44299553, 0.85558157, 0.51955588, 0.89881138, 0.62389214,\n",
       "        0.16156166, 0.66878726, 0.01755597, 0.39270117, 0.44634961,\n",
       "        0.69943851, 0.06096845, 0.51542539, 0.76042486, 0.15348453,\n",
       "        0.66226516, 0.95698009, 0.94611787, 0.40531823, 0.17901664,\n",
       "        0.12180413, 0.22515913, 0.54791384, 0.18033228, 0.31522599,\n",
       "        0.10937673, 0.96375289, 0.32556428, 0.43027792, 0.94470542,\n",
       "        0.76246375, 0.50890191, 0.13364359, 0.75193645, 0.38951345,\n",
       "        0.76094633, 0.07126273, 0.27879925, 0.16590724, 0.01507251,\n",
       "        0.58234874, 0.18209704, 0.09789784, 0.17427193, 0.7503667 ,\n",
       "        0.71175221, 0.98716769, 0.72459257, 0.45274964, 0.30919119,\n",
       "        0.56865662, 0.37256031, 0.08991063, 0.94366389, 0.21666745,\n",
       "        0.87077714, 0.21131594, 0.86087916, 0.47181924, 0.02599873,\n",
       "        0.42394336, 0.55745753, 0.50106168, 0.81558919, 0.06243185,\n",
       "        0.47489714, 0.75895893, 0.47459168, 0.86465328, 0.17738365,\n",
       "        0.59976648, 0.74978828, 0.39402951, 0.23377593, 0.07311272,\n",
       "        0.12575286, 0.40377227, 0.15808129, 0.86357197, 0.78593827,\n",
       "        0.96746246, 0.26517725, 0.74805391, 0.37216375, 0.02927186,\n",
       "        0.71217866, 0.62658647, 0.48683196, 0.16378154, 0.38269976,\n",
       "        0.65761833, 0.01034672, 0.31253571, 0.64553111, 0.80679423,\n",
       "        0.26426648, 0.53459056, 0.59863832, 0.22934797, 0.8031538 ,\n",
       "        0.33603522, 0.46392356, 0.53744811, 0.20297653, 0.88996881,\n",
       "        0.54277188, 0.98857724, 0.50068564, 0.83659986, 0.93073749,\n",
       "        0.66808711, 0.269204  , 0.35713262, 0.11455957, 0.77323997,\n",
       "        0.55207717, 0.52607213, 0.81171798, 0.78642361, 0.72016538,\n",
       "        0.84328686, 0.39485054, 0.0775158 , 0.33732999, 0.12359987,\n",
       "        0.2384211 , 0.53309257, 0.00635284, 0.81718055]),\n",
       " 48.90184478053901,\n",
       " {'grad': array([ 8.45185225e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  2.00565533e+01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          4.86260480e+00,  4.86820894e+00,  5.43256259e+00,  5.43054714e+00,\n",
       "          5.31111111e+00, -6.42784262e+00, -3.98401312e-06,  1.01252340e-06,\n",
       "          6.98818781e-06, -6.04543970e-05,  7.38467065e-06,  1.47508672e-06,\n",
       "         -4.29963620e-05, -3.17250226e-05, -4.83721152e-04,  2.49613663e-06,\n",
       "         -7.87991894e-07,  6.32525143e-06, -6.08736173e-05, -6.85034252e-06,\n",
       "         -4.81058748e-05, -7.08126890e-06, -4.53859172e-05, -1.76200388e-04,\n",
       "         -7.42607256e-03, -1.35713663e-06,  1.52837742e-06,  3.70742529e+00,\n",
       "         -4.00888212e-06,  1.00683906e-06,  7.07061076e-06, -6.05368200e-05,\n",
       "          7.39106554e-06,  1.47437618e-06, -4.30567582e-05, -3.17676552e-05,\n",
       "         -4.84332219e-04,  2.49755772e-06, -7.87991894e-07,  6.32667252e-06,\n",
       "         -6.09695405e-05, -6.86171120e-06, -4.81669815e-05, -7.08766379e-06,\n",
       "         -4.54640769e-05, -1.76464710e-04, -7.43519593e-03, -1.35784717e-06,\n",
       "          1.52979851e-06,  9.47830024e+00, -4.59081662e-06,  1.17665877e-06,\n",
       "          7.89484034e-06, -6.97504277e-05,  8.44337933e-06,  1.53832502e-06,\n",
       "         -5.11597875e-05, -3.80055099e-05, -5.55298385e-04,  2.84714474e-06,\n",
       "         -1.07860387e-06,  7.15871806e-06, -7.02257807e-05, -7.80104870e-06,\n",
       "         -5.60454794e-05, -8.07389711e-06, -5.38712186e-05, -2.10668816e-04,\n",
       "         -8.43254853e-03, -1.66764380e-06,  1.59516844e-06, -4.21008887e+00,\n",
       "         -4.56523708e-06,  1.20010668e-06,  7.91047228e-06, -6.93340496e-05,\n",
       "          8.38085157e-06,  1.56958890e-06, -5.06567233e-05, -3.75337095e-05,\n",
       "         -5.52372370e-04,  2.87201374e-06, -1.02673425e-06,  7.13740178e-06,\n",
       "         -6.98186398e-05, -7.74988962e-06, -5.56255486e-05, -8.06181788e-06,\n",
       "         -5.33610489e-05, -2.08347473e-04, -8.40599412e-03, -1.61080038e-06,\n",
       "          1.58877356e-06, -8.31077892e-01, -4.45155024e-06,  1.11057830e-06,\n",
       "          7.70583597e-06, -6.70972611e-05,  8.19895263e-06,  1.54116719e-06,\n",
       "         -4.85108842e-05, -3.58632235e-05, -5.35201394e-04,  2.76685341e-06,\n",
       "         -9.42179668e-07,  6.96331881e-06, -6.75541401e-05, -7.53672680e-06,\n",
       "         -5.36779510e-05, -7.82947041e-06, -5.11505505e-05, -1.99306527e-04,\n",
       "         -8.17619039e-03, -1.57029945e-06,  1.60511604e-06]),\n",
       "  'task': 'STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT',\n",
       "  'funcalls': 2240,\n",
       "  'nit': 3,\n",
       "  'warnflag': 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(init_param, maxiters=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8a81c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

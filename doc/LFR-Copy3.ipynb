{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c05cfc18",
   "metadata": {},
   "source": [
    "# # A1 Learning Fair Representations (LFR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc5ff6d",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d960fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.optimize as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5424c1",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e1ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"../data/compas-scores-two-years.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670edc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>race</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>score_text</th>\n",
       "      <th>sex</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>is_recid</th>\n",
       "      <th>c_jail_in</th>\n",
       "      <th>c_jail_out</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>F</td>\n",
       "      <td>Other</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-08-13 06:03:42</td>\n",
       "      <td>2013-08-14 05:41:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>F</td>\n",
       "      <td>African-American</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-26 03:45:27</td>\n",
       "      <td>2013-02-05 05:36:53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-13 04:58:34</td>\n",
       "      <td>2013-04-14 07:02:04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>Other</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-11-30 04:50:18</td>\n",
       "      <td>2013-12-01 12:28:56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Male</td>\n",
       "      <td>14</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-02-18 05:08:24</td>\n",
       "      <td>2014-02-24 12:18:30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7209</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-11-22 05:18:27</td>\n",
       "      <td>2013-11-24 02:59:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-31 07:13:54</td>\n",
       "      <td>2014-02-02 04:03:52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7211</th>\n",
       "      <td>57</td>\n",
       "      <td>F</td>\n",
       "      <td>Other</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-13 05:48:01</td>\n",
       "      <td>2014-01-14 07:49:46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7212</th>\n",
       "      <td>33</td>\n",
       "      <td>M</td>\n",
       "      <td>African-American</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Low</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-03-08 08:06:02</td>\n",
       "      <td>2014-03-09 12:18:04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>Low</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-06-28 12:16:41</td>\n",
       "      <td>2014-06-30 11:19:23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6172 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age c_charge_degree              race          age_cat score_text  \\\n",
       "0      69               F             Other  Greater than 45        Low   \n",
       "1      34               F  African-American          25 - 45        Low   \n",
       "2      24               F  African-American     Less than 25        Low   \n",
       "5      44               M             Other          25 - 45        Low   \n",
       "6      41               F         Caucasian          25 - 45     Medium   \n",
       "...   ...             ...               ...              ...        ...   \n",
       "7209   23               F  African-American     Less than 25     Medium   \n",
       "7210   23               F  African-American     Less than 25        Low   \n",
       "7211   57               F             Other  Greater than 45        Low   \n",
       "7212   33               M  African-American          25 - 45        Low   \n",
       "7213   23               F          Hispanic     Less than 25        Low   \n",
       "\n",
       "         sex  priors_count  days_b_screening_arrest  decile_score  is_recid  \\\n",
       "0       Male             0                     -1.0             1         0   \n",
       "1       Male             0                     -1.0             3         1   \n",
       "2       Male             4                     -1.0             4         1   \n",
       "5       Male             0                      0.0             1         0   \n",
       "6       Male            14                     -1.0             6         1   \n",
       "...      ...           ...                      ...           ...       ...   \n",
       "7209    Male             0                     -1.0             7         0   \n",
       "7210    Male             0                     -1.0             3         0   \n",
       "7211    Male             0                     -1.0             1         0   \n",
       "7212  Female             3                     -1.0             2         0   \n",
       "7213  Female             2                     -2.0             4         1   \n",
       "\n",
       "                c_jail_in           c_jail_out  two_year_recid  \n",
       "0     2013-08-13 06:03:42  2013-08-14 05:41:20               0  \n",
       "1     2013-01-26 03:45:27  2013-02-05 05:36:53               1  \n",
       "2     2013-04-13 04:58:34  2013-04-14 07:02:04               1  \n",
       "5     2013-11-30 04:50:18  2013-12-01 12:28:56               0  \n",
       "6     2014-02-18 05:08:24  2014-02-24 12:18:30               1  \n",
       "...                   ...                  ...             ...  \n",
       "7209  2013-11-22 05:18:27  2013-11-24 02:59:20               0  \n",
       "7210  2014-01-31 07:13:54  2014-02-02 04:03:52               0  \n",
       "7211  2014-01-13 05:48:01  2014-01-14 07:49:46               0  \n",
       "7212  2014-03-08 08:06:02  2014-03-09 12:18:04               0  \n",
       "7213  2014-06-28 12:16:41  2014-06-30 11:19:23               1  \n",
       "\n",
       "[6172 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw[['age', 'c_charge_degree', 'race', 'age_cat',\n",
    "                    'score_text', 'sex', 'priors_count', 'days_b_screening_arrest',\n",
    "                    'decile_score', 'is_recid', 'c_jail_in',\n",
    "                    'c_jail_out', 'two_year_recid']]\\\n",
    "                    .query('days_b_screening_arrest <= 30')\\\n",
    "                    .query('days_b_screening_arrest >= -30')\\\n",
    "                    .query('is_recid != -1')\\\n",
    "                    .query('c_charge_degree != \"O\"')\\\n",
    "                    .query('score_text != \"N/A\"')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35151005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5278, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unrelated columns\n",
    "df = df[(df.race=='African-American') | (df.race=='Caucasian')]\n",
    "df = df.dropna()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9deb71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "dt1 = list(map(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').date(), df['c_jail_out']))\n",
    "dt2 = list(map(lambda x: datetime.strptime(x,'%Y-%m-%d %H:%M:%S').date(), df['c_jail_in']))\n",
    "\n",
    "len_stay = [(a-b).days for a,b in zip(dt1,dt2)]\n",
    "\n",
    "df['length_of_stay'] = len_stay\n",
    "df = df.drop(['c_jail_out', 'c_jail_in'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e6c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearrange columns so y is the last column\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[:-2] + cols[-1:] + cols[-2:-1]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d64aa9",
   "metadata": {},
   "source": [
    "## Encoding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81f73a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical: ['c_charge_degree', 'race', 'age_cat', 'score_text', 'sex']\n",
      "numerical: ['age', 'priors_count', 'days_b_screening_arrest', 'decile_score', 'is_recid', 'length_of_stay']\n"
     ]
    }
   ],
   "source": [
    "label_column = ['two_year_recid']\n",
    "catogory_features = []\n",
    "numeric_features = []\n",
    "\n",
    "for col in df.columns.values:\n",
    "    if col in label_column:\n",
    "        continue\n",
    "    elif df[col].dtypes in ('int64', 'float64') :\n",
    "        numeric_features += [col]\n",
    "    else:\n",
    "        catogory_features += [col]\n",
    "        \n",
    "print(\"categorical:\", catogory_features)\n",
    "print(\"numerical:\", numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59453091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we replace categorical columns with numeric values\n",
    "df_num = df.copy()\n",
    "feat2name = {}\n",
    "encoders = {}\n",
    "\n",
    "# Use Label Encoder for categorical columns (including target column)\n",
    "for feature in catogory_features:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(df_num[feature])\n",
    "    \n",
    "    df_num[feature] = encoder.transform(df_num[feature])\n",
    "    \n",
    "    feat2name[feature] = encoder.classes_\n",
    "    encoders[feature] = encoder\n",
    "\n",
    "# Use MinMaxScaler for numerical columns     \n",
    "for feature in numeric_features:\n",
    "    val = df_num[feature].values[:, np.newaxis]\n",
    "    mms = MinMaxScaler().fit(val)\n",
    "    df_num[feature] = mms.transform(val)\n",
    "    encoders[feature] = mms\n",
    "    \n",
    "df_num = df_num.astype(float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ac76ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['African-American', 'Caucasian'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders['race'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "205f1c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5278, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a155765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>race</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>score_text</th>\n",
       "      <th>sex</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>days_b_screening_arrest</th>\n",
       "      <th>decile_score</th>\n",
       "      <th>is_recid</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.338710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.145161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  c_charge_degree  race  age_cat  score_text  sex  priors_count  \\\n",
       "1   0.258065              0.0   0.0      0.0         1.0  1.0      0.000000   \n",
       "2   0.096774              0.0   0.0      2.0         1.0  1.0      0.105263   \n",
       "6   0.370968              0.0   1.0      0.0         2.0  1.0      0.368421   \n",
       "8   0.338710              1.0   1.0      0.0         1.0  0.0      0.000000   \n",
       "10  0.145161              0.0   1.0      0.0         1.0  1.0      0.000000   \n",
       "\n",
       "    days_b_screening_arrest  decile_score  is_recid  length_of_stay  \\\n",
       "1                  0.483333      0.222222       1.0         0.01250   \n",
       "2                  0.483333      0.333333       1.0         0.00125   \n",
       "6                  0.483333      0.555556       1.0         0.00750   \n",
       "8                  0.483333      0.000000       0.0         0.00375   \n",
       "10                 0.483333      0.333333       0.0         0.00125   \n",
       "\n",
       "    two_year_recid  \n",
       "1              1.0  \n",
       "2              1.0  \n",
       "6              1.0  \n",
       "8              0.0  \n",
       "10             0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9bf0bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['African-American', 'Caucasian'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders['race'].classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c931b1",
   "metadata": {},
   "source": [
    "## Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "603597ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(df_num, test_size=0.2)\n",
    "data_train, data_val= train_test_split(data_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c911325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will define some of the constants and functions mentioned in the paper\n",
    "N = df.shape[0]  # number of samples in X\n",
    "D = df.shape[1]  # Dimension of x vector\n",
    "K = 10  # Number of prototypes represented in Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2271d8",
   "metadata": {},
   "source": [
    "## LFR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c9546a",
   "metadata": {},
   "source": [
    "The goal of LFR is to learn a good prototype set $Z$ such that:\n",
    "1. the mapping from $X_0$ to $Z$ satisfies statistical parity;\n",
    "2. the mapping to $Z$-space retains information in $X$ (except for membership in the protected set); and\n",
    "3. the induced mapping from $X$ to $Y$ (by first mapping each $x$ probabilistically to $Z$-space, and then mapping $Z$ to $Y$) is close to f.\n",
    "\n",
    "Each of these aims corresponds to a term in the objective function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef07e6a",
   "metadata": {},
   "source": [
    "## (1) Define "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8011e8f",
   "metadata": {},
   "source": [
    "We define $M_{nk}$ as the probability that $x_n$ maps to $v_k$.\n",
    "\n",
    "So,\n",
    "$$M_{nk} = P(Z=k|x_n) \\space\\space \n",
    "=\\frac{exp(-d(x_n, v_k))}{\\sum_{k=1}^K exp(-d(x_n, v_k))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04ce34",
   "metadata": {},
   "source": [
    "###  Calculates the euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496429af",
   "metadata": {},
   "source": [
    "$$d(x_n, v_k, \\alpha) = \\sum^D_{d=1} \\alpha_d (x_{nd} - v_{kd})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1f1a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(x1, x2, alpha):\n",
    "    \"\"\"\n",
    "        Calculates the euclidean distance between x1 and x2 with feature weights alpha\n",
    "        x1: First vector in X vector space (D, 1)\n",
    "        x2: Second vector in X vector space (D, 1)\n",
    "        alpha: weight vector for each of the features (D, 1)\n",
    "    \"\"\"\n",
    "    x1 = np.matrix(x1)\n",
    "    x2 = np.matrix(x2)\n",
    "    alpha = np.matrix(alpha)\n",
    "#     print(x1, x2, alpha)\n",
    "#     print(np.multiply(np.multiply((x1 - x2), (x1 - x2)),alpha))\n",
    "    return sum(np.multiply(np.multiply((x1 - x2), (x1 - x2)), alpha))[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c52fd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test \n",
    "d(np.matrix([1,2,3]).T, np.matrix([0,0,0]).T, np.matrix([1,1,2]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b74ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save time for later, we will cache the distance map between all inputs X_i \n",
    "# and current prototypes V_k\n",
    "def d_map(X, V, alpha):\n",
    "    \"\"\"\n",
    "        Returns a 2D matrix with shape (N, K) with each cell (i, j) \n",
    "            distance from input x_i to prototype v_j with weighted features\n",
    "        X: Input matrix (N, D)\n",
    "        V: Prototype matrix (K, D)\n",
    "        alpha: weight vector for each of the features (D, 1)\n",
    "    \"\"\"\n",
    "    distance_map = np.zeros((X.shape[0], V.shape[0]))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(V.shape[0]):\n",
    "            distance_map[i, j] = d(X[i, :], V[j, :], alpha)\n",
    "            \n",
    "    return distance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e714ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[162.,   8.],\n",
       "       [ 98.,   0.],\n",
       "       [ 32.,  18.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "d_map(np.matrix([[1,2],[3,4],[6,7]]), np.matrix([[10,2],[3,40]]), np.matrix([[1.0],[1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d5eaf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_nk(X, n, V, k, alpha, dist_map, summation):\n",
    "    \"\"\"\n",
    "        Calculate the prob of X_n is classified to kth prototype using softmax\n",
    "        X: Input matrix (N, D)\n",
    "        n: the nth input to calculate the prob for\n",
    "        V: prototype matrix (K, D)\n",
    "        k: the kth prototype to classify for\n",
    "        alpha: weight vector for each of the features (D, 1)\n",
    "    \"\"\"\n",
    "    p = 0\n",
    "    exponent = np.exp(-1 * dist_map[n, k])\n",
    "    p = exponent / summation\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3534b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save time later, we will cache the probs of each x mapped to k\n",
    "def M_map(X, V, alpha):\n",
    "    \"\"\"\n",
    "        Return the prob of each x mapping to a prototype v (N, K)\n",
    "        X: Input matrix (N, D)\n",
    "        V: Prototype matrix (K, D)\n",
    "        alpha: weight vector for each of the features (D, 1)\n",
    "    \"\"\"\n",
    "    M = np.zeros((X.shape[0], V.shape[0]))\n",
    "    \n",
    "    dist_map = d_map(X, V, alpha)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(V.shape[0]):\n",
    "            summation = 0\n",
    "            for k_idx in range(V.shape[0]):\n",
    "                summation += np.exp(-1 * dist_map[i, k_idx])\n",
    "            # To avoid value error\n",
    "            if (summation == 0): \n",
    "                summation = 0.000001\n",
    "            M[i, j] = M_nk(X, i, V, j, alpha, dist_map, summation)\n",
    "    return M\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f616f0",
   "metadata": {},
   "source": [
    "$$M_k =\\mathop{\\mathbb{E}}_{x \\in X} P(Z=k|x)= \\frac{1}{|X|} \\sum_{n \\in X} M_{nk}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "574d24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_sub_k(M_sub_map):\n",
    "    \"\"\"\n",
    "        Calculate estimated prob of mapping to k for a subset M_map. (K,)\n",
    "        M_sub_map: prob of each x mapping to a prototype (N0, K)\n",
    "    \"\"\"\n",
    "    Ms = np.zeros(M_sub_map.shape[1])\n",
    "    \n",
    "    for k in range(M_sub_map.shape[1]):\n",
    "        for n in range(M_sub_map.shape[0]):\n",
    "            Ms[k] += M_sub_map[n, k]\n",
    "        Ms[k] /= M_sub_map.shape[0]\n",
    "    return Ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e70a9",
   "metadata": {},
   "source": [
    "## (2) Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37cc00",
   "metadata": {},
   "source": [
    "## \n",
    "<h1 align = \"center\">$Total \\space Loss = A_x * L_x + A_y * L_y + A_z * L_z 􏰂􏰀􏰀$<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657df402",
   "metadata": {},
   "source": [
    "where $A_x, A_y, A_z$ are hyper-parameters governing the trade-off between the system desiderata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6042fb",
   "metadata": {},
   "source": [
    "## <div align='center' ><font size='5'>$L_x = \\sum_{n=1}^N (x_n - \\hat{x}_n)^2$</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3121c",
   "metadata": {},
   "source": [
    "where $$\\hat{x}_n = \\sum^K_{k=1}M_{nk}v_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df127ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_x(X, x_hats):\n",
    "    \"\"\"\n",
    "        Loss term for goodness of the prototype.\n",
    "        X: input matrix (N, D)\n",
    "        x_hats: x estimates (N, D)\n",
    "    \"\"\"\n",
    "    Lx = 0\n",
    "    for n in range(X.shape[0]):\n",
    "        for d in range(X.shape[1]):\n",
    "            Lx += (X[n, d] - x_hats[n, d]) * (X[n, d] - x_hats[n, d])\n",
    "    return Lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14d59851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_hats(M, V):\n",
    "    \"\"\"\n",
    "        Return a matrix of reconstructed x through M \n",
    "            using each of the prototypes. (N, D)\n",
    "        M: M_map output (N, K)\n",
    "        V: Prototy$$\\hat{x}_n = \\sum^K_{k=1}M_{nk}v_k$$pe matrix (K, D)\n",
    "    \"\"\"\n",
    "    return np.matmul(M, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8aeb49",
   "metadata": {},
   "source": [
    "## <div align='center' ><font size='5'>$L_y = \\sum_{n=1}^N -y_n log \\hat{y}_n - (1-y_n)log(1- \\hat{y}_n)$</font></div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499a058",
   "metadata": {},
   "source": [
    "$$\\hat{y}_n = \\sum^K_{k=1} M_{nk}w_k \\\\\n",
    "0< w_k <1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20c28526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_y(ys, y_hats):\n",
    "    \"\"\"\n",
    "        Loss term for accuracy of the model\n",
    "        ys: Gound-truth ## $L_y = \\sum_{n=1}^N -y_n log \\hat{y}_n - (1-y_n)log(1- \\hat{y}_n)$label of X (N, 1)\n",
    "        y_hats: y estimates (N, 1)\n",
    "    \"\"\"\n",
    "    Ly = 0\n",
    "    for n in range(ys.shape[0]): \n",
    "        Ly += (-1 * ys[n] * np.log(y_hats[n]) - (1 - ys[n]) * (np.log(1 - y_hats[n])))\n",
    "    return Ly[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db6a4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hats(M, w):\n",
    "    \"\"\"\n",
    "        Return matrix of final estimates of each input through M and trained w.\n",
    "        M: M_map output (N, K)\n",
    "        w: Model weight between 0 and 1 (K, 1)\n",
    "    \"\"\"\n",
    "    y_hat = np.zeros(M.shape[0])\n",
    "    for n in range(M.shape[0]):\n",
    "        for k in range(M.shape[1]):\n",
    "            y_hat[n] += (M[n, k] * w[k])\n",
    "        # Clipping estimates to (0, 1)\n",
    "        y_hat[n] = 0.000001 if y_hat[n] <= 0 else y_hat[n]\n",
    "        y_hat[n] = 0.999999 if y_hat[n] >= 1 else y_hat[n]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ec3c4",
   "metadata": {},
   "source": [
    "## <div align='center' ><font size='5'>$L_z􏰂􏰀= 􏰀\\sum_{k=1}^K|M_k^+-M_k^-|$􏰀</font></div> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7015a6f",
   "metadata": {},
   "source": [
    "In order to achieve statistical parity, we want to ensure, which can be estimated using the training data as:\n",
    "\n",
    "$$M_k^+ = M_K^-   \\space   \\space \\space \\forall k$$\n",
    "\n",
    "where\n",
    "\n",
    "$$M_k^+ =\\mathop{\\mathbb{E}}_{x \\in X^+} P(Z=k|x)= \\frac{1}{|X^+|} \\sum_{n \\in X^+} M_{nk}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c937001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_z(M_sens, M_nonsens):\n",
    "    \"\"\"\n",
    "        Loss term for fairness.\n",
    "        M_sens: M_sub_k for sensitive data (1, K)\n",
    "        M_nonsens: M_sub_k for non-sensitive data (1, K)\n",
    "    \"\"\"\n",
    "    Lz= 0.0\n",
    "    \n",
    "    for k in range(M_sens.shape[0]):\n",
    "          Lz += abs(M_sens[k] - M_nonsens[k])\n",
    "    return Lz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee933bc9",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71694b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute classification\n",
    "def compute_error(y_hat, y):\n",
    "    # we will split y_hat by 0.5\n",
    "    clipped = np.clip(y_hat, 0, 1)\n",
    "    rounded = np.around(clipped)\n",
    "    return np.abs(rounded - y).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9048c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error(np.array([0.4, 0.6, 0.7]), np.array([1.0, 1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04fe2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFR():\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data,\n",
    "        test_data,\n",
    "        label_column,\n",
    "        sensitive_column,\n",
    "        privileged_group,\n",
    "        k,\n",
    "        A_x,\n",
    "        A_y,\n",
    "        A_z\n",
    "    ):\n",
    "        self.k = k\n",
    "        self.A_x = A_x\n",
    "        self.A_y = A_y\n",
    "        self.A_z = A_z\n",
    "        \n",
    "        self.__name__ = str(k) + \" \" + str(A_x) + \" \" + str(A_y) + \" \" + str(A_z) \n",
    "        \n",
    "        self.curr_iters = 0\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.label_column = label_column\n",
    "        self.sensitive_column = sensitive_column\n",
    "        self.privileged_group = privileged_group\n",
    "        \n",
    "        train_copy = train_data.copy()\n",
    "        train_copy.drop(columns=label_column)\n",
    "        self.X = np.matrix(train_copy.to_numpy())\n",
    "        self.y = np.matrix(train_data[label_column].to_numpy()).T\n",
    "        \n",
    "        sens = train_data[sensitive_column]\n",
    "        priv_idx = np.array(np.where(sens==privileged_group))[0].flatten()\n",
    "        nonpriv_idx = np.array(np.where(sens!=privileged_group))[0].flatten()\n",
    "        self.X_plus = self.X[priv_idx,:]\n",
    "        self.y_plus = self.y[priv_idx,:]\n",
    "        self.X_minus = self.X[nonpriv_idx,:]\n",
    "        self.y_minus = self.y[nonpriv_idx,:]\n",
    "        \n",
    "    def fit(self, init_params, maxiters=100):\n",
    "        bnd = []\n",
    "        for i, k2 in enumerate(init_params):\n",
    "            if i < self.X.shape[1] * 2 or i >= self.X.shape[1] * 2 + self.k:\n",
    "                bnd.append((None, None))\n",
    "            else:\n",
    "                bnd.append((0, 1))\n",
    "        self.curr_param = init_params\n",
    "#         return\n",
    "        return optim.fmin_l_bfgs_b(self.forward, x0=init_params, epsilon=1e-5, \n",
    "                          bounds = bnd, approx_grad=True, maxfun=maxiters, maxiter=maxiters)\n",
    "        \n",
    "    def forward(self, params, return_params=False):\n",
    "        \"\"\"\n",
    "            \n",
    "        \"\"\"\n",
    "        self.curr_iters += 1\n",
    "        \n",
    "#         print(\"N_priv\")\n",
    "\n",
    "        N_priv, D = self.X_plus.shape\n",
    "        N_nonpriv, _ = self.X_minus.shape\n",
    "\n",
    "#         print(\"Extract\")\n",
    "        # Extract all params\n",
    "        alpha_priv, alpha_nonpriv, w, V = self.extract_param(params)\n",
    "\n",
    "#         print(\"Ms\")\n",
    "        M_k_p = M_map(self.X_plus, V, alpha_priv)\n",
    "        M_k_n = M_map(self.X_minus, V, alpha_nonpriv)\n",
    "\n",
    "#         print(\"Lz\")\n",
    "        Lz = L_z(M_sub_k(M_k_p), M_sub_k(M_k_n))\n",
    "\n",
    "#         print(\"Xhats\")\n",
    "        # To save time, we will just sum the two groups up\n",
    "        x_hats_p = x_hats(M_k_p, V)\n",
    "        x_hats_n = x_hats(M_k_n, V)\n",
    "#         print(\"Lx\")\n",
    "        L_x_p = L_x(self.X_plus, x_hats_p)\n",
    "        L_x_n = L_x(self.X_minus, x_hats_n)\n",
    "\n",
    "        Lx = L_x_p + L_x_n\n",
    "\n",
    "#         print(\"Yhats\")\n",
    "        y_hats_p = y_hats(M_k_p, w)\n",
    "        y_hats_n = y_hats(M_k_n, w)\n",
    "#         print(\"Ly\")\n",
    "        L_y_p = L_y(self.y_plus, y_hats_p)\n",
    "        L_y_n = L_y(self.y_minus, y_hats_n)\n",
    "\n",
    "        Ly = L_y_p + L_y_n\n",
    "\n",
    "#         print(\"Loss\", Lx, Ly, Lz)\n",
    "        loss = (self.A_x * Lx) + (self.A_y * Ly) + (self.A_z * Lz)\n",
    "\n",
    "        self.curr_param = params\n",
    "        if self.curr_iters % 50 == 0:\n",
    "            print(\n",
    "                \"model:\", self.__name__,\n",
    "                \"step:\", self.curr_iters, \n",
    "                \"loss:\", loss, \n",
    "                \"Lx:\", Lx, \n",
    "                \"Ly:\", Ly, \n",
    "                \"Lz:\", Lz)\n",
    "#             print(\"params y_hats_p, y_hats_n, M_k_p, M_k_n, loss:\",\n",
    "#                  y_hats_p, y_hats_n, M_k_p, M_k_n, loss)\n",
    "            # Predict\n",
    "            test_copy = self.test_data.copy()\n",
    "            test_copy.drop(columns=[label_column])\n",
    "\n",
    "            X_test = np.matrix(test_copy.to_numpy())\n",
    "            y_test = np.matrix(self.test_data[label_column].to_numpy()).T\n",
    "        \n",
    "            M_k_p_val = M_map(X_test, V, alpha_priv)\n",
    "            \n",
    "            y_hat = y_hats(M_k_p_val, w)\n",
    "            print(\"current error:\", compute_error(y_hat, y_test))\n",
    "\n",
    "\n",
    "        if return_params:\n",
    "            return y_hats_p, y_hats_n, M_k_p, M_k_n, loss\n",
    "        else:\n",
    "            return loss\n",
    "        \n",
    "    def extract_param(self, params):\n",
    "        \n",
    "        _, D = self.X_plus.shape\n",
    "        # Extract all params\n",
    "        alpha_priv = params[:D].T\n",
    "        alpha_nonpriv = params[D:2*D].T\n",
    "\n",
    "        w = params[2*D:2*D+self.k]\n",
    "        V = np.matrix(params[(2*D)+self.k:]).reshape((self.k, D))\n",
    "        return alpha_priv, alpha_nonpriv, w, V\n",
    "        \n",
    "    def predict(self, X_test, priv=True):\n",
    "        alpha_priv, alpha_nonpriv, w, V = self.extract_param(self.curr_param)\n",
    "        \n",
    "        if (priv):\n",
    "            M_k_p = M_map(X_test, V, alpha_priv)\n",
    "        else:\n",
    "            M_k_p = M_map(X_test, V, alpha_nonpriv)\n",
    "        \n",
    "        return y_hats(M_k_p, w)\n",
    "    \n",
    "    def predict_with_param(self, X_test, param, priv=True):\n",
    "        alpha_priv, alpha_nonpriv, w, V = self.extract_param(param)\n",
    "        \n",
    "        if (priv):\n",
    "            M_k_p = M_map(X_test, V, alpha_priv)\n",
    "        else:\n",
    "            M_k_p = M_map(X_test, V, alpha_nonpriv)\n",
    "        \n",
    "        return y_hats(M_k_p, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce5c1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(init_param, maxiters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4993fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see the model has reached minima at step 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f11de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column=\"two_year_recid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4b8a81c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Vars to store results\n",
    "# cval_errs = {} # Mean validation errors\n",
    "# train_time = {} # Training time\n",
    "# # Best Model\n",
    "# best_model = None\n",
    "# # Best Validation Error\n",
    "# best_err = sys.maxsize\n",
    "\n",
    "# # Model selection \n",
    "# KS = [5]#, 10]\n",
    "# Axs = [0.00000001]#, 1, 1000000]\n",
    "# Ays = [0.01]#, 1, 1000]\n",
    "# Azs = [1000]#, 10000, 1000000]\n",
    "\n",
    "# train_data = data_train.copy()\n",
    "\n",
    "# for K in KS:\n",
    "#     init_param = np.random.uniform(size=df.shape[1] * 2 + K + df.shape[1] * K)\n",
    "#     for Ax in Axs:\n",
    "#         for Ay in Ays:    \n",
    "#             for Az in Azs:\n",
    "#                 kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "#                 y_err = []\n",
    "\n",
    "#                 start = time.time()\n",
    "\n",
    "#                 # Cross Validaiton\n",
    "#                 for train_index, val_index in kf.split(train_data):\n",
    "#                 #     print(\"TRAIN:\", train_index, \"VAL:\", val_index)\n",
    "#                     train_copy = train_data.copy()\n",
    "#                     train_copy.drop(columns=[label_column])\n",
    "#                     train_df = train_copy.iloc[train_index]\n",
    "                    \n",
    "#                     X_val = np.matrix(train_copy.iloc[val_index].to_numpy())\n",
    "#                     y_val = np.matrix(train_data.iloc[val_index][label_column].to_numpy()).T\n",
    "\n",
    "#                     model = LFR(\n",
    "#                         train_df,\n",
    "#                         data_val,\n",
    "#                         \"two_year_recid\",\n",
    "#                         \"race\",\n",
    "#                         1,\n",
    "#                         K,\n",
    "#                         Ax,\n",
    "#                         Ay,\n",
    "#                         Az\n",
    "#                     )\n",
    "#                     model.fit(init_param, maxiters=500)\n",
    "                    \n",
    "#                     y_hat = model.predict(X_val)\n",
    "# #                     print(y_hat)\n",
    "#                     y_err.append(compute_error(y_hat, y_val))\n",
    "\n",
    "#                 end = time.time()\n",
    "\n",
    "#                 print(str(K), str(Ax), str(Ay), str(Az), \"mean val MAE:\", np.mean(y_err))\n",
    "#                 print(\"Time lapsed\", str((end - start)*1000))\n",
    "\n",
    "#                 # add to dict\n",
    "#                 cval_errs[model.__name__] = np.mean(y_err)\n",
    "#                 train_time[model.__name__] = (end - start)*1000\n",
    "#                 if np.mean(y_err) < best_err:\n",
    "#                     best_model = model\n",
    "#                     best_err = np.mean(y_err)\n",
    "#                     best_errs = y_err\n",
    "                \n",
    "# print(best_model.__name__, best_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85ccd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33d13792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1dab3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 1e-08 0.01 1000'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 5\n",
    "label_column = \"two_year_recid\"\n",
    "model = LFR(\n",
    "    data_train,\n",
    "    data_val,\n",
    "    label_column,\n",
    "    \"race\",\n",
    "    1,\n",
    "    K,\n",
    "    0.00000001,\n",
    "    0.01,\n",
    "    1000\n",
    ")\n",
    "model.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9453de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init_param = np.random.uniform(size=df.shape[1] * 2 + K + df.shape[1] * K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe7c059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.5388257575757576\n"
     ]
    }
   ],
   "source": [
    "model.forward(init_param)\n",
    "test_copy = data_test.copy()\n",
    "X_test = np.matrix(test_copy.to_numpy())\n",
    "y_test = np.matrix(data_test[label_column].to_numpy()).T\n",
    "y_hat = model.predict(X_test)\n",
    "print(\"error:\", compute_error(y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04fe59da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 5 1e-08 0.01 1000 step: 50 loss: 51.71868568473771 Lx: 12282.67876256688 Ly: 2708.8774915032964 Lz: 0.02462978794291712\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 100 loss: 41.21464650695299 Lx: 12296.75780309468 Ly: 2681.1014009324526 Lz: 0.014403509530050429\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 150 loss: 41.21464650685327 Lx: 12296.747831476372 Ly: 2681.1014009324526 Lz: 0.014403509530050429\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 200 loss: 42.794464103048696 Lx: 12318.61055110675 Ly: 2562.600275182554 Lz: 0.017168338165117647\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 250 loss: 42.79446410304962 Lx: 12318.610643361735 Ly: 2562.600275182554 Lz: 0.017168338165117647\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 300 loss: 35.53218167132115 Lx: 12306.010676180164 Ly: 2627.5815812224096 Lz: 0.009256242798990288\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 350 loss: 35.53216678452355 Lx: 12306.00822925071 Ly: 2627.5758078696404 Lz: 0.00925628564574485\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 400 loss: 41.591413645434805 Lx: 12335.518962774764 Ly: 2516.8432536024948 Lz: 0.016422857754220227\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 450 loss: 41.5914136455236 Lx: 12335.527842532603 Ly: 2516.8432536024948 Lz: 0.016422857754220227\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 500 loss: 35.902016192751724 Lx: 12314.08861956466 Ly: 2592.77961891865 Lz: 0.009974096862679027\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 550 loss: 35.084260946110334 Lx: 12308.60673245716 Ly: 2616.0448098887355 Lz: 0.008923689761155651\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 600 loss: 35.0842609460111 Lx: 12308.596809334998 Ly: 2616.0448098887355 Lz: 0.008923689761155651\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 650 loss: 27.449663151560422 Lx: 12285.223105635094 Ly: 2527.1848436903947 Lz: 0.0021776918624254182\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 700 loss: 27.449663151560365 Lx: 12285.223099816581 Ly: 2527.1848436903947 Lz: 0.0021776918624254182\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 750 loss: 24.4346537536865 Lx: 12265.712896119345 Ly: 2438.1060053925144 Lz: 5.347104263239699e-05\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 800 loss: 24.434624563403702 Lx: 12265.711658134076 Ly: 2438.1026537625994 Lz: 5.3475368661126677e-05\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 850 loss: 24.28128206950901 Lx: 12264.98402789867 Ly: 2397.14792767994 Lz: 0.0003096801428693341\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 900 loss: 24.28128206960189 Lx: 12264.99331588444 Ly: 2397.14792767994 Lz: 0.0003096801428693341\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 950 loss: 23.93332747570427 Lx: 12266.72905990711 Ly: 2379.0684516407105 Lz: 0.0001425202920065649\n",
      "current error: 0.5372781065088758\n",
      "model: 5 1e-08 0.01 1000 step: 1000 loss: 23.49829018647274 Lx: 12272.650299995083 Ly: 2337.634377993555 Lz: 0.00012182368003418875\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1050 loss: 23.498290186376636 Lx: 12272.640689815598 Ly: 2337.634377993555 Lz: 0.00012182368003418875\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1100 loss: 23.436593943029784 Lx: 12275.213654823585 Ly: 2336.7758465905113 Lz: 6.871272498812364e-05\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1150 loss: 23.436593943029465 Lx: 12275.213622747466 Ly: 2336.7758465905113 Lz: 6.871272498812364e-05\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1200 loss: 23.409971687868865 Lx: 12273.87441152709 Ly: 2336.6395918857406 Lz: 4.345303026734304e-05\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1250 loss: 23.40995292328517 Lx: 12273.872991547032 Ly: 2336.63955486492 Lz: 4.3434635906058894e-05\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1300 loss: 23.41126347931182 Lx: 12269.273758988908 Ly: 2337.4026953004704 Lz: 3.7113833569524246e-05\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1350 loss: 23.411263479404578 Lx: 12269.283034931232 Ly: 2337.4026953004704 Lz: 3.7113833569524246e-05\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1400 loss: 23.396402603146047 Lx: 12271.776751125093 Ly: 2336.8601245666223 Lz: 2.76786397123141e-05\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1450 loss: 23.376072076808303 Lx: 12271.125113418762 Ly: 2336.729201968884 Lz: 8.657345868329225e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1500 loss: 23.376072076712198 Lx: 12271.115502760073 Ly: 2336.729201968884 Lz: 8.657345868329225e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1550 loss: 23.374411058724213 Lx: 12269.933071716818 Ly: 2336.670292048855 Lz: 7.585438904944564e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1600 loss: 23.374411058723886 Lx: 12269.933039087799 Ly: 2336.670292048855 Lz: 7.585438904944564e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1650 loss: 23.370459053106735 Lx: 12270.120591011095 Ly: 2336.645864211489 Lz: 3.877709785937711e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1700 loss: 23.370452797310346 Lx: 12270.119266974647 Ly: 2336.6458514090677 Lz: 3.871582026998555e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1750 loss: 23.373914634285185 Lx: 12269.230239968452 Ly: 2336.5818209190015 Lz: 7.973732792765365e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1800 loss: 23.373914634377964 Lx: 12269.239518137261 Ly: 2336.5818209190015 Lz: 7.973732792765365e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1850 loss: 23.369322847804188 Lx: 12269.882118544796 Ly: 2336.6289363724923 Lz: 2.9107852580756433e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1900 loss: 23.36901913209924 Lx: 12269.311967907684 Ly: 2336.6435081516825 Lz: 2.4613574627352364e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 1950 loss: 23.369019132003125 Lx: 12269.302356155065 Ly: 2336.6435081516825 Lz: 2.4613574627352364e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2000 loss: 23.368433179928324 Lx: 12269.343584349328 Ly: 2336.6379124635673 Lz: 1.9313618568050916e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2050 loss: 23.368433179927997 Lx: 12269.343551919816 Ly: 2336.6379124635673 Lz: 1.9313618568050916e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2100 loss: 23.36941195478437 Lx: 12269.748336657347 Ly: 2336.637683998019 Lz: 2.9124173208106008e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2150 loss: 23.369408223992 Lx: 12269.747021306945 Ly: 2336.637724187096 Lz: 2.908284650826598e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2200 loss: 23.36760296736489 Lx: 12269.483616394326 Ly: 2336.632716356188 Lz: 1.153108966839289e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2250 loss: 23.367602967457678 Lx: 12269.492895457795 Ly: 2336.632716356188 Lz: 1.153108966839289e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2300 loss: 23.372494972947866 Lx: 12270.240565845725 Ly: 2336.6440579479236 Lz: 5.9316910629714226e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2350 loss: 23.367301074406743 Lx: 12269.609923180906 Ly: 2336.6285110583553 Lz: 8.932677239559261e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2400 loss: 23.367301074310628 Lx: 12269.600311922268 Ly: 2336.6285110583553 Lz: 8.932677239559261e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2450 loss: 23.36665033341457 Lx: 12269.589838295502 Ly: 2336.6250657921755 Lz: 2.769795944268516e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2500 loss: 23.366650333414242 Lx: 12269.589805844462 Ly: 2336.6250657921755 Lz: 2.769795944268516e-07\n",
      "current error: 0.46272189349112425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 5 1e-08 0.01 1000 step: 2550 loss: 23.36801231847854 Lx: 12269.520378797206 Ly: 2336.611392292585 Lz: 1.7757003488994005e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2600 loss: 23.368006894155094 Lx: 12269.519067941103 Ly: 2336.6114117468815 Lz: 1.7700814956000954e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2650 loss: 23.366557973743163 Lx: 12269.57804181509 Ly: 2336.6229675762866 Lz: 2.0560219987642014e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2700 loss: 23.366557973835953 Lx: 12269.587320799681 Ly: 2336.6229675762866 Lz: 2.0560219987642014e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2750 loss: 23.366556860158816 Lx: 12269.589722082117 Ly: 2336.6241467008717 Lz: 1.9269725287651696e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2800 loss: 23.366523782551635 Lx: 12269.58201621593 Ly: 2336.6235284646355 Lz: 1.658020851169617e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2850 loss: 23.36652378245552 Lx: 12269.572404838907 Ly: 2336.6235284646355 Lz: 1.658020851169617e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2900 loss: 23.36672249035956 Lx: 12269.573593064399 Ly: 2336.623024452491 Lz: 3.6955009871664224e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 2950 loss: 23.366722490359233 Lx: 12269.573560631265 Ly: 2336.623024452491 Lz: 3.6955009871664224e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3000 loss: 23.36645663417813 Lx: 12269.5798756301 Ly: 2336.6234086693403 Lz: 9.98516859751497e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3050 loss: 23.366453098174198 Lx: 12269.578563921694 Ly: 2336.6234294111027 Lz: 9.61082775308153e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3100 loss: 23.36747727139662 Lx: 12269.677365612111 Ly: 2336.6248583030183 Lz: 1.1059915927813524e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3150 loss: 23.36747727148941 Lx: 12269.686644453639 Ly: 2336.6248583030183 Lz: 1.1059915927813524e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3200 loss: 23.36649803189981 Lx: 12269.598882047128 Ly: 2336.6236046411914 Lz: 1.3928949907371369e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3250 loss: 23.36642573203516 Lx: 12269.584256502087 Ly: 2336.6234591693114 Lz: 6.844449948473219e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3300 loss: 23.36642573193905 Lx: 12269.574645123841 Ly: 2336.6234591693114 Lz: 6.844449948473219e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3350 loss: 23.36874740899962 Lx: 12269.602233287513 Ly: 2336.634776207188 Lz: 2.2769509054054726e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3400 loss: 23.368747408999294 Lx: 12269.602200880236 Ly: 2336.634776207188 Lz: 2.2769509054054726e-06\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3450 loss: 23.36660256303301 Lx: 12269.586579441908 Ly: 2336.624826392727 Lz: 2.3160323994741994e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3500 loss: 23.36660466151784 Lx: 12269.585267559421 Ly: 2336.6248463216343 Lz: 2.3350244882092142e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3550 loss: 23.366427429530155 Lx: 12269.583541266553 Ly: 2336.6236940538874 Lz: 6.779315586680212e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3600 loss: 23.366427429622945 Lx: 12269.592820238304 Ly: 2336.6236940538874 Lz: 6.779315586680212e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3650 loss: 23.366415041307135 Lx: 12269.588978133712 Ly: 2336.6235519312254 Lz: 5.682610509927244e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3700 loss: 23.366397353702265 Lx: 12269.59026299871 Ly: 2336.6241032187118 Lz: 3.36256125166301e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3750 loss: 23.366397353606153 Lx: 12269.580651653181 Ly: 2336.6241032187118 Lz: 3.36256125166301e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3800 loss: 23.366385348532223 Lx: 12269.58728684391 Ly: 2336.623822492874 Lz: 2.4427730616061183e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3850 loss: 23.3663853485319 Lx: 12269.587254397089 Ly: 2336.623822492874 Lz: 2.4427730616061183e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3900 loss: 23.366775018836805 Lx: 12269.625859617528 Ly: 2336.625947959254 Lz: 3.9284298566921727e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 3950 loss: 23.366781698867207 Lx: 12269.624547033434 Ly: 2336.6259694511327 Lz: 3.9930811041033465e-07\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4000 loss: 23.366400633591663 Lx: 12269.591284742412 Ly: 2336.6241028322634 Lz: 3.6909356182501796e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4050 loss: 23.366400633684453 Lx: 12269.600563724849 Ly: 2336.6241028322634 Lz: 3.6909356182501796e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4100 loss: 23.366371066263458 Lx: 12269.593458842086 Ly: 2336.6239107058827 Lz: 9.263270039916449e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4150 loss: 23.366371999275334 Lx: 12269.589482100098 Ly: 2336.6239426889056 Lz: 9.876491458005177e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4200 loss: 23.366371999179222 Lx: 12269.579870749141 Ly: 2336.6239426889056 Lz: 9.876491458005177e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4250 loss: 23.36638629791569 Lx: 12269.596489698515 Ly: 2336.6237439691263 Lz: 2.6162259525452924e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4300 loss: 23.366386297915366 Lx: 12269.596457250304 Ly: 2336.6237439691263 Lz: 2.6162259525452924e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4350 loss: 23.366373765424154 Lx: 12269.591029215313 Ly: 2336.6239088594466 Lz: 1.198091939547119e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4400 loss: 23.36637853026524 Lx: 12269.589717259892 Ly: 2336.6239299998697 Lz: 1.6534369373655267e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4450 loss: 23.36637211189021 Lx: 12269.58867211043 Ly: 2336.623933332788 Lz: 1.0082675611089442e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4500 loss: 23.366372111983 Lx: 12269.59795108984 Ly: 2336.623933332788 Lz: 1.0082675611089442e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4550 loss: 23.366372022038153 Lx: 12269.59410599461 Ly: 2336.6239407939743 Lz: 9.918157350963241e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4600 loss: 23.366372003640105 Lx: 12269.589494205462 Ly: 2336.6239423248358 Lz: 9.884496804390963e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4650 loss: 23.366372003543994 Lx: 12269.579882854548 Ly: 2336.6239423248358 Lz: 9.884496804390963e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4700 loss: 23.36637200010968 Lx: 12269.589484401025 Ly: 2336.6239426197017 Lz: 9.878017820375007e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4750 loss: 23.366372000109354 Lx: 12269.589451953309 Ly: 2336.6239426197017 Lz: 9.878017820375007e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4800 loss: 23.36637212309377 Lx: 12269.58948253618 Ly: 2336.6239550417 Lz: 9.87678194785957e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4850 loss: 23.366377964900103 Lx: 12269.58817061767 Ly: 2336.62397593353 Lz: 1.5509683093384297e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4900 loss: 23.36637199929431 Lx: 12269.588360313657 Ly: 2336.6239426863985 Lz: 9.876546719356227e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 4950 loss: 23.3663719993871 Lx: 12269.597639293224 Ly: 2336.6239426863985 Lz: 9.876546719356227e-09\n",
      "current error: 0.46272189349112425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 5 1e-08 0.01 1000 step: 5000 loss: 23.366371999328212 Lx: 12269.594042972953 Ly: 2336.6239426884194 Lz: 9.87650358719172e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 5050 loss: 23.36637199927373 Lx: 12269.589482103067 Ly: 2336.623942688798 Lz: 9.87649093064924e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 5100 loss: 23.36637199917762 Lx: 12269.579870752106 Ly: 2336.623942688798 Lz: 9.87649093064924e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 5150 loss: 23.366371999282002 Lx: 12269.589482111442 Ly: 2336.623942688551 Lz: 9.876501672057003e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 5200 loss: 23.36637199928168 Lx: 12269.589449663777 Ly: 2336.623942688551 Lz: 9.876501672057003e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 5250 loss: 23.366372122934877 Lx: 12269.589482104282 Ly: 2336.6239550547652 Lz: 9.876492401694748e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 5300 loss: 23.366377964746608 Lx: 12269.588170185736 Ly: 2336.623975946536 Lz: 1.5509399542423807e-08\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 5350 loss: 23.366371999263965 Lx: 12269.588360234178 Ly: 2336.6239426887932 Lz: 9.876492429450323e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 5400 loss: 23.366371999356755 Lx: 12269.59763921383 Ly: 2336.6239426887932 Lz: 9.876492429450323e-09\n",
      "current error: 0.46272189349112425\n",
      "model: 5 1e-08 0.01 1000 step: 5450 loss: 23.36637199931934 Lx: 12269.594042960303 Ly: 2336.623942688798 Lz: 9.87649093064924e-09\n",
      "current error: 0.46272189349112425\n"
     ]
    }
   ],
   "source": [
    "op = model.fit(init_param, maxiters=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15549663",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy = data_test.copy()\n",
    "test_copy.drop(columns=label_column)\n",
    "test_X = np.matrix(test_copy.to_numpy())\n",
    "test_y = np.matrix(data_test[label_column].to_numpy()).T\n",
    "privileged_group = 1\n",
    "sens = data_test[\"race\"]\n",
    "priv_idx = np.array(np.where(sens==privileged_group))[0].flatten()\n",
    "nonpriv_idx = np.array(np.where(sens!=privileged_group))[0].flatten()\n",
    "test_X_plus = test_X[priv_idx,:]\n",
    "test_y_plus = test_y[priv_idx,:]\n",
    "test_X_minus = test_X[nonpriv_idx,:]\n",
    "test_y_minus = test_y[nonpriv_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8667a5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priv error: 0.3752913752913753\n",
      "non priv error: 0.5199362041467305\n",
      "overall error: 0.46117424242424243\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "\n",
    "y_hat_priv = model.predict(test_X_plus)\n",
    "y_hat_nonpriv = model.predict(test_X_minus)\n",
    "print(\"priv error:\", compute_error(y_hat_priv, test_y_plus))\n",
    "print(\"non priv error:\", compute_error(y_hat_nonpriv, test_y_minus))\n",
    "print(\"overall error:\", compute_error(np.concatenate((y_hat_nonpriv, y_hat_priv)), \n",
    "                                      np.concatenate((test_y_minus, test_y_plus))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "239d148d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0860446 , 0.48391339, 0.14352093, 0.2260606 , 0.06334933,\n",
       "        0.62182417, 0.17559923, 0.5987066 , 0.18218406, 0.6286742 ,\n",
       "        0.04404399, 0.69881137, 0.21485092, 0.46821003, 0.90281652,\n",
       "        0.02003778, 0.0995888 , 0.46292028, 0.9506006 , 0.32273576,\n",
       "        0.39858341, 0.62061651, 0.95609486, 0.51230091, 0.11617536,\n",
       "        0.70181324, 0.21057602, 0.58888266, 0.76104689, 0.18014743,\n",
       "        0.08054532, 0.00893841, 0.63332043, 0.37238256, 0.10919819,\n",
       "        0.36424165, 0.44532985, 0.66402941, 0.85402702, 0.95541148,\n",
       "        0.56231683, 0.18015636, 0.65636991, 0.39717764, 0.64549661,\n",
       "        0.3397862 , 0.80643798, 0.16736284, 0.47670641, 0.04396053,\n",
       "        0.81379983, 0.32009883, 0.51548235, 0.16167711, 0.27604056,\n",
       "        0.64820925, 0.19231329, 0.20385995, 0.38391196, 0.78971249,\n",
       "        0.58531676, 0.36953208, 0.31819169, 0.47968703, 0.78070219,\n",
       "        0.18015894, 0.61398203, 0.88346662, 0.69913591, 0.08984385,\n",
       "        0.6508417 , 0.72624455, 0.78769812, 0.18715203, 0.22588234,\n",
       "        0.72355318, 0.81678137, 0.16168611, 0.85644431, 0.03415686,\n",
       "        0.45084892, 0.84425949, 0.28715857, 0.09035437, 0.54641595,\n",
       "        0.31267792, 0.55704861, 0.64015822, 0.65607821]),\n",
       " 23.36637199927373,\n",
       " {'grad': array([ 1.04859824e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          4.41647771e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          1.23646178e-02,  1.23656822e-02,  1.01707897e-02,  1.23660090e-02,\n",
       "          1.01718470e-02, -4.72165116e-01,  2.16928697e-06, -3.23296945e-08,\n",
       "         -1.50954804e-06, -9.61577484e-06, -4.92761387e-06,  4.56097382e-06,\n",
       "          1.31734623e-06, -1.12194698e-06,  6.25632879e-07,  8.16093859e-06,\n",
       "          2.58104649e-06,  4.03910927e-01,  2.16928697e-06, -3.23296945e-08,\n",
       "         -1.50954804e-06, -9.61577484e-06, -4.92761387e-06,  4.56097382e-06,\n",
       "          1.31734623e-06, -1.12194698e-06,  6.25632879e-07,  8.16093859e-06,\n",
       "          2.58104649e-06, -2.92499218e-01,  2.16893170e-06, -3.23296945e-08,\n",
       "         -1.51132440e-06, -9.61115632e-06, -4.92512697e-06,  4.55919746e-06,\n",
       "          1.31663569e-06, -1.12265752e-06,  6.24211793e-07,  8.15703061e-06,\n",
       "          2.57855959e-06,  5.23484227e-01,  2.16928697e-06, -3.23296945e-08,\n",
       "         -1.50954804e-06, -9.61577484e-06, -4.92761387e-06,  4.56097382e-06,\n",
       "          1.31734623e-06, -1.12194698e-06,  6.25632879e-07,  8.16093859e-06,\n",
       "          2.58104649e-06,  5.96547192e-01,  2.16893170e-06, -3.23296945e-08,\n",
       "         -1.51132440e-06, -9.61115632e-06, -4.92512697e-06,  4.55919746e-06,\n",
       "          1.31663569e-06, -1.12265752e-06,  6.24211793e-07,  8.15703061e-06,\n",
       "          2.57855959e-06]),\n",
       "  'task': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH',\n",
       "  'funcalls': 5490,\n",
       "  'nit': 27,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d159c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.array([0.76944432, 0.44201403, 0.75157162, 0.93536497, 0.47968939,\n",
    "        0.96171022, 0.43356675, 0.94331112, 0.25556224, 0.77026722,\n",
    "        0.7024049 , 0.38050666, 0.50203774, 0.28388108, 0.96967332,\n",
    "        0.61517901, 0.35110927, 0.72631221, 0.18616932, 0.72570045,\n",
    "        0.31150333, 0.39246312, 0.47864698, 0.97388464, 0.9097853 ,\n",
    "        0.90803786, 0.9396043 , 0.59208026, 0.75504674, 0.69620556,\n",
    "        0.6140349 , 0.14928146, 0.96429274, 0.73807815, 0.78801899,\n",
    "        0.54089478, 0.76872627, 0.97631913, 0.88428702, 0.1670892 ,\n",
    "        0.3597392 , 0.73960804, 0.11786876, 0.29632785, 0.90119911,\n",
    "        0.93501812, 0.05906133, 0.52597812, 0.698342  , 0.1376443 ,\n",
    "        0.62903134, 0.25388978, 0.8276283 , 0.63269536, 0.72433145,\n",
    "        0.90955815, 0.24890163, 0.69352591, 0.04085464, 0.12961094,\n",
    "        0.93352539, 0.56594276, 0.58493874, 0.79196367, 0.81338372,\n",
    "        0.51545163, 0.43163521, 0.22640088, 0.95132431, 0.63158748,\n",
    "        0.55031502, 0.03016215, 0.27204168, 0.05419203, 0.3846093 ,\n",
    "        0.28693374, 0.23654671, 0.28052912, 0.99309054, 0.82368537,\n",
    "        0.22821508, 0.26639806, 0.69196584, 0.82223622, 0.52312309,\n",
    "        0.66008233, 0.69931073, 0.45596373, 0.79808916])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d102e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 0.5388257575757576\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "test_copy = data_test.copy()\n",
    "test_copy.drop(columns=[label_column])\n",
    "\n",
    "X_test = np.matrix(test_copy.to_numpy())\n",
    "y_test = np.matrix(data_test[label_column].to_numpy()).T\n",
    "y_hat = model.predict_with_param(X_test, out)\n",
    "print(\"error:\", compute_error(y_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3affd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
